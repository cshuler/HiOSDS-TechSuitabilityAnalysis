{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aaade89",
   "metadata": {},
   "source": [
    "# Prepare Master Parcel Attribute Table (MPAT) Inputs\n",
    "*Author: Alemarie Ceria*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00590534",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook creates the analysis-ready inputs used to build the MPAT in `notebooks/02_build_mpat.ipynb`. Separating the preparation avoids reruns of expensive tasks. Each prep step writes a single canonical output and skips work if the output already exists, unless `FORCE=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3804091",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "The following breaks down the data directories used in this notebook:\n",
    "\n",
    "- `data/01_inputs/source/`: vendor/original downloads\n",
    "- `data/01_inputs/prepared/`: validated/corrected/mosaicked + CRS-harmonized, used by MPAT\n",
    "- `data/01_inputs/prepared/review_queue/`: outputs for manual review; may be incorporated into prepared inputs in later iterations but are not read directly by the MPAT build notebook\n",
    "- `data/01_inputs/prepared/_archive/`: outdated prepared layers moved here into dated subfolders (e.g., `_archive/20260111/`) when schema changes or reprocessing occurs; preserves audit trail without cluttering active directory\n",
    "- `data/02_interim/`: temporary working layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbeffe3",
   "metadata": {},
   "source": [
    "### Outputs\n",
    "\n",
    "- `data/01_inputs/prepared/`\n",
    "    - `parcels_hi_higp_validated_32604.gpkg`\n",
    "    - `cesspools_inventory_hi_hcpt_corrected_32604.gpkg`\n",
    "    - `dem_hi_pacioos_mosaic_32604.tif`\n",
    "    - `watertable_hi_hcpt_mosaic_32604.tif`\n",
    "    - `annual_rainfall_hi_hcpt_32604.tif`\n",
    "    - `coastline_hi_op_32604.gpkg`\n",
    "    - `sma_hi_op_32604.gpkg`\n",
    "- `data/01_inputs/prepared/review_queue/`\n",
    "    - `cesspools_inventory_hi_hcpt_review_queue_32604.gpkg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f39f5",
   "metadata": {},
   "source": [
    "### Input / Output Contract\n",
    "\n",
    "- This notebook reads only from `data/01_inputs/source/` and any existing prepared inputs.\n",
    "- Canonical prepared inputs are written to `data/01_inputs/prepared/`.\n",
    "- Features requiring manual review are written to `data/01_inputs/prepared/review_queue/` and are not read directly by the MPAT build notebook.\n",
    "- Temporary working layers created during preparation are written to `data/02_interim/` and may be deleted and rebuilt.\n",
    "- The MPAT build notebook (`02_build_mpat.ipynb`) reads only from `data/01_inputs/prepared/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b924a",
   "metadata": {},
   "source": [
    "### Workflow Overview\n",
    "\n",
    "The main tasks of this notebook are: \n",
    "\n",
    "1. Validating the parcel layer and dissolving known duplicate parcel with cesspools\n",
    "2. Correcting TMK string values and geometries in the cesspool layer\n",
    "3. Mosaicking the multi-raster water table layers\n",
    "4. Reprojecting all layers to `EPSG:32604` to ensure consistent spatial operations\n",
    "5. Export vectors as geopackages and rasters to GeoTIFFs for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b2799",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaadd01",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dcd1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# import yaml\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "from shapely import make_valid\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterstats import point_query\n",
    "import folium\n",
    "from folium.plugins import GroupedLayerControl\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3f989",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"crs\": {\n",
    "        \"target\": \"EPSG:32604\"\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"source\": \"data/01_inputs/source\",\n",
    "        \"prepared\": \"data/01_inputs/prepared\",\n",
    "        \"review_queue\": \"data/01_inputs/prepared/_review_queue\",\n",
    "        \"interim\": \"data/02_interim\"\n",
    "    },\n",
    "    \"inputs\": {\n",
    "        \"source\": {\n",
    "            \"cesspools\": \"cesspools_inventory_hi_hcpt/2025CP_Expld_V2_clean.shp\",\n",
    "            \"dem_dir\": \"dem_hi_pacioos\",\n",
    "            \"watertable_dir\": \"watertable_hi_hcpt\",\n",
    "            \"slope_dir\": \"slope_hi_hcpt\",\n",
    "            \"streams\": \"streams_hi_hcpt/Streams_prj.shp\",\n",
    "            \"mun_wells\": \"wells_hi_hcpt/CWRM_Wells_MUN_prj.shp\",\n",
    "            \"dom_wells\": \"wells_hi_hcpt/CWRM_Wells_DOM_prj.shp\",\n",
    "            \"rainfall\": \"annual_rainfall_hi_hcpt/Rain_inann.tif\",\n",
    "            \"coastline\": \"coastline_hi_op/coastline.shp\",\n",
    "            \"sma\": \"sma_hi_op/sma.shp\",\n",
    "        },\n",
    "        \"interim\": {\n",
    "            \"parcels_repaired\": \"tempspace/parcels_hi_higp_repaired_32604/parcels_hi_higp_repaired_32604.shp\",\n",
    "        }\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        # Base mpat layers\n",
    "        \"parcels_preprocessed\": \"parcels_hi_higp_preprocessed_32604.gpkg\",\n",
    "        \"cesspools_corrected\": \"cesspools_inventory_hi_hcpt_corrected_32604.gpkg\",\n",
    "        \"cesspools_review\": \"cesspools_inventory_hi_hcpt_review_queue_32604.gpkg\",\n",
    "\n",
    "        # Rasters\n",
    "        \"dem_mosaic\": \"dem_hi_pacioos_mosaic_32604.tif\",\n",
    "        \"watertable_mosaic\": \"watertable_hi_hcpt_mosaic_32604.tif\",\n",
    "        \"rainfall\": \"annual_rainfall_hi_hcpt_32604.tif\",\n",
    "\n",
    "        # Vectors\n",
    "        \"streams\":   \"streams_hi_hcpt_32604.gpkg\",\n",
    "        \"mun_wells\": \"wells_hi_hcpt_mun_32604.gpkg\",\n",
    "        \"dom_wells\": \"wells_hi_hcpt_dom_32604.gpkg\",\n",
    "        \"coastline\": \"coastline_hi_op_32604.gpkg\",\n",
    "        \"sma\": \"sma_hi_op_32604.gpkg\",\n",
    "    },\n",
    "    \"fields\": {\n",
    "        \"parcels\": {\n",
    "            \"keep_cols\": [\"tmk_txt\", \"island\"],\n",
    "            \"rename_map\": {\"tmk_txt\": \"tmk_parcels\"},\n",
    "            \"tmk_col\": \"tmk_parcels\",  # Name after renaming, used for QA\n",
    "            \"target_dup_tmk\": \"139019004\"\n",
    "        },\n",
    "        \"cesspools\": {\n",
    "            \"keep_cols\": [\n",
    "                \"tmk\", \"class_i\", \"class_ii\", \"class_iii\", \"class_iv\", \n",
    "                \"class_v\", \"osds_qty\", \"bedroom\", \"effluent\", \"nitrogen_f\", \"phosphorus\"\n",
    "            ],\n",
    "            \"rename_map\": {\"tmk\": \"tmk_cps\", \"nitrogen_f\": \"nitrogen\"},\n",
    "            \"tmk_col\": \"tmk_cps\",      # Name after renaming\n",
    "            \"filter_col\": \"class_iv\",  # Filter to class_iv != 0\n",
    "            \"filter_value\": 0\n",
    "        }\n",
    "    },\n",
    "    \"thresholds\": {\n",
    "        \"tmk_correction_distance_m\": 25\n",
    "    },\n",
    "    \"run\": {\n",
    "        \"force\": False,\n",
    "        \"write_log\": True,\n",
    "        \"dry_run\": False,\n",
    "        \"stop_on_error\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fcad7",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0fcb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project root\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "# Data directories\n",
    "SOURCE = PROJECT_ROOT / CONFIG[\"paths\"][\"source\"]\n",
    "PREP = PROJECT_ROOT / CONFIG[\"paths\"][\"prepared\"]\n",
    "REVIEW = PROJECT_ROOT / CONFIG[\"paths\"][\"review_queue\"]\n",
    "INTERIM = PROJECT_ROOT / CONFIG[\"paths\"][\"interim\"]\n",
    "\n",
    "# CRS\n",
    "TARGET_CRS = CONFIG[\"crs\"][\"target\"]\n",
    "\n",
    "# Inputs\n",
    "IN_SOURCE = {k: SOURCE / v for k, v in CONFIG[\"inputs\"][\"source\"].items()}\n",
    "IN_INTERIM = {k: INTERIM / v for k, v in CONFIG[\"inputs\"][\"interim\"].items()}\n",
    "\n",
    "# Outputs\n",
    "OUT_PREP = {k: PREP / v for k, v in CONFIG[\"outputs\"].items() if \"review\" not in k}\n",
    "OUT_REVIEW = {\n",
    "    \"cesspools_review\": REVIEW / CONFIG[\"outputs\"][\"cesspools_review\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a4965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export configs to config.yaml\n",
    "# yaml_path = PROJECT_ROOT / \"config.yaml\"\n",
    "# yaml_path.write_text(yaml.safe_dump(CONFIG, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77dcc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config.yaml\n",
    "# CONFIG = yaml.safe_load((PROJECT_ROOT / \"config.yaml\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e1434",
   "metadata": {},
   "source": [
    "### Run Controls\n",
    "\n",
    "For execution choices. Allows us to define:\n",
    "\n",
    "- Skip (assumes existing file matches current processing steps) <br>or rerun expensive step (do when output schema is changed)\n",
    "- Print progress/QA messages\n",
    "- Run partial code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9efb8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE = CONFIG[\"run\"][\"force\"]                    # rerun steps even if outputs exist\n",
    "WRITE_LOG = CONFIG[\"run\"][\"write_log\"]            # print progress and QA messages\n",
    "DRY_RUN = CONFIG[\"run\"][\"dry_run\"]                # define steps but do not write outputs\n",
    "STOP_ON_ERROR = CONFIG[\"run\"][\"stop_on_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45f7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example step pattern\n",
    "# if output_path.exists() and not FORCE:\n",
    "#     log(\"Output exists; skipping.\")\n",
    "# else:\n",
    "#     try:\n",
    "#         if DRY_RUN:\n",
    "#             log(f\"[DRY RUN] Would write: {output_path}\")\n",
    "#         else:\n",
    "#             # do work\n",
    "#             write_output(...)\n",
    "#             log(f\"Wrote: {output_path}\")\n",
    "#     except Exception as e:\n",
    "#         log(f\"ERROR: {e}\")\n",
    "#         if STOP_ON_ERROR:\n",
    "#             raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5f3ce",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15b1f0",
   "metadata": {},
   "source": [
    "#### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df776bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg: str) -> None:\n",
    "    \"\"\"\n",
    "    Print a formatted progress or QA message if logging is enabled.\n",
    "\n",
    "    This helper centralizes all user-facing messages so verbosity can be\n",
    "    controlled globally via the WRITE_LOG run flag.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    log(\"Loaded parcels layer\")\n",
    "    \"\"\"\n",
    "    if WRITE_LOG:\n",
    "        print(f\"\\n--- {msg} ---\")\n",
    "\n",
    "def should_run(out_path: Path, force: bool) -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether a processing step should execute.\n",
    "\n",
    "    A step should run if:\n",
    "    - the expected output does not yet exist, or\n",
    "    - FORCE is True (explicit override).\n",
    "\n",
    "    This supports safe re-runs of the notebook without unintentionally\n",
    "    recomputing expensive steps.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    if should_run(out_path, FORCE):\n",
    "        run_step()\n",
    "    else:\n",
    "        log(\"Output exists; skipping.\")\n",
    "    \"\"\"\n",
    "    return force or (not out_path.exists())\n",
    "\n",
    "@contextmanager\n",
    "def timed_step(label: str):\n",
    "    \"\"\"\n",
    "    Context manager to time an expensive processing step.\n",
    "\n",
    "    Timing messages are logged only when WRITE_LOG is True.\n",
    "    Uses wall-clock time (time.perf_counter) for accurate runtime reporting.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    with timed_step(\"Geometry validation\"):\n",
    "        parcels[\"geometry\"] = parcels[\"geometry\"].apply(make_valid)\n",
    "    \"\"\"\n",
    "    if not WRITE_LOG:\n",
    "        # No timing or logging; just run the block\n",
    "        yield\n",
    "        return\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        elapsed = time.perf_counter() - start\n",
    "        if elapsed < 60:\n",
    "            log(f\"{label} completed in {elapsed:.2f} seconds\")\n",
    "        else:\n",
    "            log(f\"{label} completed in {elapsed / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df26c2",
   "metadata": {},
   "source": [
    "#### I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3ef0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vector(path, layer_name: str):\n",
    "    \"\"\"\n",
    "    Read a vector dataset into a GeoDataFrame.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    parcels = read_vector(in_path, \"parcels_repaired\")\n",
    "    \"\"\"\n",
    "    log(f\"Reading {layer_name}: {path}\")\n",
    "    with timed_step(f\"Read {layer_name}\"):\n",
    "        gdf = gpd.read_file(path).clean_names()\n",
    "    log(f\"{layer_name}: rows={len(gdf):,} | crs={gdf.crs}\")\n",
    "    return gdf\n",
    "\n",
    "def write_vector_gpkg(gdf, out_path, layer_name: str):\n",
    "    \"\"\"\n",
    "    Write a GeoDataFrame to a GeoPackage using Fiona (stable CRS writing in this env).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    write_vector_gpkg(parcels, OUT_PREP[\"parcels_validated\"], \"parcels_validated\")\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with timed_step(f\"Write {layer_name} (GPKG)\"):\n",
    "        gdf.to_file(out_path, layer=layer_name, driver=\"GPKG\", engine=\"fiona\")\n",
    "    log(f\"Wrote: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2446d97",
   "metadata": {},
   "source": [
    "#### Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07a427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_vector_crs(gdf, target_crs: str, layer_name: str):\n",
    "    \"\"\"\n",
    "    Ensure a GeoDataFrame is in the target CRS.\n",
    "\n",
    "    Reprojects only if the CRS differs. Logs the action taken.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    parcels = ensure_vector_crs(parcels, TARGET_CRS, \"parcels\")\n",
    "    \"\"\"\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(f\"{layer_name} CRS is missing. Cannot reproject safely.\")\n",
    "\n",
    "    target = gpd.GeoSeries([], crs=target_crs).crs  # normalize to CRS object\n",
    "\n",
    "    if gdf.crs != target:\n",
    "        log(f\"{layer_name}: reprojecting from {gdf.crs} to {target_crs}\")\n",
    "        with timed_step(f\"Reproject {layer_name}\"):\n",
    "            return gdf.to_crs(target)\n",
    "\n",
    "    log(f\"{layer_name}: already in target CRS ({target_crs})\")\n",
    "    return gdf\n",
    "\n",
    "def validate_geometry_if_needed(gdf, layer_name: str):\n",
    "    \"\"\"\n",
    "    Validate geometries using shapely.make_valid only when invalid geometries exist.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    parcels = validate_geometry_if_needed(parcels, \"parcels\")\n",
    "    \"\"\"\n",
    "    n_invalid_before = int((~gdf.is_valid).sum())\n",
    "    log(f\"{layer_name}: invalid geometries (before): {n_invalid_before:,}\")\n",
    "\n",
    "    if n_invalid_before > 0:\n",
    "        with timed_step(f\"{layer_name}: make_valid\"):\n",
    "            invalid_mask = ~gdf.is_valid\n",
    "            gdf.loc[invalid_mask, \"geometry\"] = (\n",
    "                gdf.loc[invalid_mask, \"geometry\"].apply(make_valid)\n",
    "            )\n",
    "\n",
    "    n_invalid_after = int((~gdf.is_valid).sum())\n",
    "    log(f\"{layer_name}: invalid geometries (after):  {n_invalid_after:,}\")\n",
    "\n",
    "    if n_invalid_after > 0:\n",
    "        log(f\"WARNING: {layer_name} still has invalid geometries after make_valid.\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def check_geometry_quality(gdf, layer_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check for null, empty, and invalid geometries.\n",
    "    \n",
    "    Returns dict with counts for logging/decisions.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    geom_qa = check_geometry_quality(parcels, \"parcels\")\n",
    "    \"\"\"\n",
    "    qa = {\n",
    "        \"null\": int(gdf.geometry.isna().sum()),\n",
    "        \"empty\": int(gdf.geometry.is_empty.sum()),\n",
    "        \"invalid\": int((~gdf.is_valid).sum()),\n",
    "    }\n",
    "    log(f\"{layer_name} geometry QA: null={qa['null']:,}, empty={qa['empty']:,}, invalid={qa['invalid']:,}\")\n",
    "    return qa\n",
    "\n",
    "def summarize_vector(path: Path, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get summary stats for a vector file (GPKG/SHP/etc).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    stats = summarize_vector(OUT_PREP[\"parcels_validated\"], \"parcels_validated\")\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(path)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"path\": path.name,\n",
    "        \"crs\": str(gdf.crs),\n",
    "        \"rows\": len(gdf),\n",
    "        \"columns\": len(gdf.columns),\n",
    "    }\n",
    "\n",
    "def dissolve_duplicate_tmk_polygons(\n",
    "    parcels: gpd.GeoDataFrame,\n",
    "    tmk_col: str,\n",
    "    target_tmk: str,\n",
    "    layer_name: str = \"parcels\",\n",
    ") -> gpd.GeoDataFrame:\n",
    "    if parcels is None or parcels.empty:\n",
    "        log(f\"{layer_name}: empty; skipping targeted dissolve.\")\n",
    "        return parcels\n",
    "\n",
    "    if tmk_col not in parcels.columns:\n",
    "        raise KeyError(f\"{layer_name}: missing TMK column '{tmk_col}'\")\n",
    "    if \"geometry\" not in parcels.columns:\n",
    "        raise KeyError(f\"{layer_name}: missing geometry column\")\n",
    "\n",
    "    gdf = parcels.copy()\n",
    "    gdf[tmk_col] = gdf[tmk_col].astype(\"string\")\n",
    "    target_tmk = str(target_tmk)\n",
    "\n",
    "    target_mask = (gdf[tmk_col] == target_tmk)\n",
    "    existing_n = int(target_mask.sum())\n",
    "    log(f\"{layer_name}: targeted dissolve TMK={target_tmk} occurrences={existing_n:,}\")\n",
    "\n",
    "    if existing_n <= 1:\n",
    "        log(f\"{layer_name}: nothing to dissolve (<=1 occurrence).\")\n",
    "        return gdf\n",
    "\n",
    "    dup_parts = gdf.loc[target_mask].copy()\n",
    "    dup_parts = dup_parts.loc[dup_parts.geometry.notna() & ~dup_parts.geometry.is_empty].copy()\n",
    "\n",
    "    if len(dup_parts) < 2:\n",
    "        log(f\"{layer_name}: duplicates exist but <2 non-empty geometries; skipping.\")\n",
    "        return gdf\n",
    "\n",
    "    geoms = dup_parts.geometry.tolist()\n",
    "    with timed_step(f\"{layer_name}: dissolve {target_tmk} (pairwise union)\"):\n",
    "        merged_geom = geoms[0]\n",
    "        for geom in geoms[1:]:\n",
    "            merged_geom = merged_geom.union(geom)\n",
    "\n",
    "    if merged_geom is None or merged_geom.is_empty:\n",
    "        raise ValueError(f\"{layer_name}: union produced empty geometry for TMK {target_tmk}\")\n",
    "\n",
    "    parcels_keep = gdf.loc[~target_mask].copy()\n",
    "\n",
    "    merged_row = dup_parts.iloc[[0]].copy()\n",
    "    merged_row.geometry = [merged_geom]\n",
    "\n",
    "    # IMPORTANT: assign a plain string scalar (avoid dtype weirdness)\n",
    "    merged_row[tmk_col] = target_tmk\n",
    "\n",
    "    out = pd.concat([parcels_keep, merged_row], ignore_index=True)\n",
    "    out = gpd.GeoDataFrame(out, geometry=\"geometry\", crs=gdf.crs)\n",
    "\n",
    "    # IMPORTANT: compare against plain string, handle NA safely\n",
    "    final_n = int((out[tmk_col].astype(\"string\").fillna(\"\") == target_tmk).sum())\n",
    "    if final_n != 1:\n",
    "        raise ValueError(f\"{layer_name}: expected 1 row for {target_tmk}, got {final_n}\")\n",
    "\n",
    "    log(f\"{layer_name}: dissolved TMK {target_tmk} to 1 row.\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3232930d",
   "metadata": {},
   "source": [
    "#### Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e6f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_raster_crs(input_dir: Path, layer_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check CRS status of all rasters in a directory.\n",
    "    \n",
    "    Use before mosaicking to identify files needing CRS assignment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : Path\n",
    "        Directory containing .tif files.\n",
    "    layer_name : str\n",
    "        Name for logging.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Keys: 'valid' (list of filenames with CRS), \n",
    "              'missing' (list of filenames without CRS),\n",
    "              'crs_values' (dict of filename: CRS string)\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    crs_check = check_raster_crs(SOURCE / \"watertable_hi_hcpt\", \"watertable\")\n",
    "    # Returns: {'valid': ['oahu_wt.tif', ...], 'missing': ['hawaii_wt.tif'], 'crs_values': {...}}\n",
    "    \"\"\"\n",
    "    tif_files = sorted(input_dir.glob(\"*.tif\"))\n",
    "    log(f\"{layer_name}: checking CRS for {len(tif_files)} rasters\")\n",
    "    \n",
    "    result = {\"valid\": [], \"missing\": [], \"crs_values\": {}}\n",
    "    \n",
    "    for tif_path in tif_files:\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            if src.crs is None:\n",
    "                result[\"missing\"].append(tif_path.name)\n",
    "                result[\"crs_values\"][tif_path.name] = None\n",
    "                log(f\"  {tif_path.name}: NO CRS\")\n",
    "            else:\n",
    "                result[\"valid\"].append(tif_path.name)\n",
    "                result[\"crs_values\"][tif_path.name] = str(src.crs)\n",
    "                log(f\"  {tif_path.name}: {src.crs}\")\n",
    "    \n",
    "    if result[\"missing\"]:\n",
    "        log(f\"{layer_name}: {len(result['missing'])} raster(s) missing CRS\")\n",
    "    else:\n",
    "        log(f\"{layer_name}: all rasters have valid CRS\")\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def set_raster_crs(\n",
    "    in_path: Path,\n",
    "    out_path: Path,\n",
    "    epsg: int,\n",
    "    layer_name: str,\n",
    "    force: bool = False,\n",
    "    dry_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Assign a CRS to a raster that has no spatial reference.\n",
    "    \n",
    "    This does NOT reproject the data; it only sets the CRS metadata.\n",
    "    Use when you know what CRS the raster should have but it's missing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_path : Path\n",
    "        Input raster path (missing CRS).\n",
    "    out_path : Path\n",
    "        Output raster path (with CRS assigned).\n",
    "    epsg : int\n",
    "        EPSG code to assign (e.g., 4326, 32604).\n",
    "    layer_name : str\n",
    "        Name for logging.\n",
    "    force : bool\n",
    "        Overwrite output if exists.\n",
    "    dry_run : bool\n",
    "        Log actions without writing.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to output raster with CRS assigned.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    # Fix raster with missing CRS before mosaicking\n",
    "    set_raster_crs(\n",
    "        in_path=SOURCE / \"watertable_hi_hcpt\" / \"hawaii_wt.tif\",\n",
    "        out_path=INTERIM / \"hawaii_wt_crs_fixed.tif\",\n",
    "        epsg=4326,\n",
    "        layer_name=\"watertable_hawaii\",\n",
    "        force=False,\n",
    "        dry_run=False,\n",
    "    )\n",
    "    \"\"\"\n",
    "    if out_path.exists() and not force:\n",
    "        log(f\"{layer_name}: output exists; skipping CRS assignment.\")\n",
    "        return out_path\n",
    "    \n",
    "    with rasterio.open(in_path) as src:\n",
    "        if src.crs is not None:\n",
    "            log(f\"WARNING: {layer_name} already has CRS={src.crs}. Skipping.\")\n",
    "            return in_path\n",
    "        \n",
    "        log(f\"{layer_name}: assigning EPSG:{epsg} to raster with no CRS\")\n",
    "        \n",
    "        if dry_run:\n",
    "            log(f\"[DRY RUN] Would write: {out_path}\")\n",
    "            return out_path\n",
    "        \n",
    "        profile = src.profile.copy()\n",
    "        profile.update(crs=rasterio.crs.CRS.from_epsg(epsg))\n",
    "        \n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with timed_step(f\"Set CRS for {layer_name}\"):\n",
    "            with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "    \n",
    "    log(f\"Wrote: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "def mosaic_rasters(\n",
    "    input_dir: Path,\n",
    "    out_path: Path,\n",
    "    layer_name: str,\n",
    "    force: bool = False,\n",
    "    dry_run: bool = False,\n",
    "):\n",
    "    if out_path.exists() and not force:\n",
    "        log(f\"{layer_name}: mosaic exists; skipping.\")\n",
    "        return out_path\n",
    "\n",
    "    tif_files = sorted(input_dir.glob(\"*.tif\"))\n",
    "    log(f\"{layer_name}: found {len(tif_files)} input rasters\")\n",
    "\n",
    "    if len(tif_files) == 0:\n",
    "        raise FileNotFoundError(f\"No .tif files found in {input_dir}\")\n",
    "\n",
    "    datasets = []\n",
    "    crs_values = set()\n",
    "    nodata_values = set()\n",
    "\n",
    "    for tif_path in tif_files:\n",
    "        src = rasterio.open(tif_path)\n",
    "        if src.crs is None:\n",
    "            for ds in datasets:\n",
    "                ds.close()\n",
    "            raise ValueError(f\"{layer_name}: missing CRS in {tif_path.name}. Fix before mosaicking.\")\n",
    "\n",
    "        datasets.append(src)\n",
    "        crs_values.add(str(src.crs))\n",
    "        nodata_values.add(src.nodata)\n",
    "\n",
    "    # IMPORTANT: merge assumes inputs are already in the same CRS/grid space\n",
    "    if len(crs_values) > 1:\n",
    "        for ds in datasets:\n",
    "            ds.close()\n",
    "        raise ValueError(f\"{layer_name}: inputs have multiple CRS values; reproject first. Found: {sorted(crs_values)}\")\n",
    "\n",
    "    # Choose nodata (prefer existing; otherwise set a safe DEM nodata)\n",
    "    nodata = next((v for v in nodata_values if v is not None), None)\n",
    "    if nodata is None:\n",
    "        nodata = -9999.0\n",
    "        log(f\"WARNING {layer_name}: inputs have no nodata; using {nodata} for mosaic output.\")\n",
    "\n",
    "    if dry_run:\n",
    "        log(f\"[DRY RUN] Would mosaic {len(datasets)} rasters to {out_path}\")\n",
    "        for ds in datasets:\n",
    "            ds.close()\n",
    "        return out_path\n",
    "\n",
    "    with timed_step(f\"Mosaic {layer_name}\"):\n",
    "        mosaic_data, mosaic_transform = merge(\n",
    "            datasets,\n",
    "            nodata=nodata,\n",
    "        )\n",
    "\n",
    "        mosaic_crs = datasets[0].crs\n",
    "        mosaic_dtype = datasets[0].dtypes[0]\n",
    "\n",
    "        for ds in datasets:\n",
    "            ds.close()\n",
    "\n",
    "        profile = {\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": mosaic_data.shape[1],\n",
    "            \"width\": mosaic_data.shape[2],\n",
    "            \"count\": mosaic_data.shape[0],\n",
    "            \"dtype\": mosaic_dtype,\n",
    "            \"crs\": mosaic_crs,\n",
    "            \"transform\": mosaic_transform,\n",
    "            \"nodata\": nodata,\n",
    "        }\n",
    "\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "            dst.write(mosaic_data)\n",
    "\n",
    "    log(f\"Wrote: {out_path}\")\n",
    "    return out_path\n",
    "    \n",
    "def ensure_raster_crs(\n",
    "    in_path,\n",
    "    out_path,\n",
    "    target_epsg: int,\n",
    "    layer_name: str,\n",
    "    force: bool = False,\n",
    "    dry_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensure a raster is in the target CRS by writing a CRS-harmonized copy.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - If out_path exists and force=False: skip and return out_path\n",
    "    - If input CRS matches target EPSG: copy raster to out_path\n",
    "    - Otherwise: reproject raster to target EPSG and write out_path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_path : pathlib.Path\n",
    "        Input raster path.\n",
    "    out_path : pathlib.Path\n",
    "        Output raster path (prepared input).\n",
    "    target_epsg : int\n",
    "        Target EPSG code (e.g., 32604).\n",
    "    layer_name : str\n",
    "        Name used in log messages.\n",
    "    force : bool\n",
    "        If True, overwrite/recompute even if out_path exists.\n",
    "    dry_run : bool\n",
    "        If True, log actions but do not write outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        Path to the raster in the target CRS (out_path).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ensure_raster_crs(\n",
    "        in_path=SOURCE / \"annual_rainfall_hi_hcpt\" / \"Rain_inann.tif\",\n",
    "        out_path=OUT_PREP[\"rainfall\"],\n",
    "        target_epsg=32604,\n",
    "        layer_name=\"rainfall\",\n",
    "        force=FORCE,\n",
    "        dry_run=DRY_RUN,\n",
    "    )\n",
    "    \"\"\"\n",
    "    if out_path.exists() and not force:\n",
    "        log(f\"{layer_name}: output exists; skipping CRS ensure step.\")\n",
    "        return out_path\n",
    "\n",
    "    with rasterio.open(in_path) as src:\n",
    "        if src.crs is None:\n",
    "            raise ValueError(f\"{layer_name} CRS is missing. Cannot reproject safely.\")\n",
    "\n",
    "        target_crs = rasterio.crs.CRS.from_epsg(target_epsg)\n",
    "\n",
    "        # Ensure output directory exists (unless dry run)\n",
    "        if not dry_run:\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Case 1: Already in target CRS -> copy\n",
    "        # -----------------------------\n",
    "        if src.crs == target_crs:\n",
    "            log(f\"{layer_name}: CRS already EPSG:{target_epsg}\")\n",
    "\n",
    "            if dry_run:\n",
    "                log(f\"[DRY RUN] Would copy: {in_path} -> {out_path}\")\n",
    "                return out_path\n",
    "\n",
    "            with timed_step(f\"Copy {layer_name} (already in target CRS)\"):\n",
    "                profile = src.profile.copy()\n",
    "                data = src.read()\n",
    "\n",
    "                with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "                    dst.write(data)\n",
    "\n",
    "            log(f\"Wrote: {out_path}\")\n",
    "            return out_path\n",
    "\n",
    "        # -----------------------------\n",
    "        # Case 2: CRS differs -> reproject\n",
    "        # -----------------------------\n",
    "        log(f\"{layer_name}: reprojecting raster -> EPSG:{target_epsg}\")\n",
    "\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, target_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "\n",
    "        nodata = src.nodata\n",
    "        if nodata is None:\n",
    "            nodata = -9999.0\n",
    "            log(f\"WARNING {layer_name}: src nodata missing; using {nodata} for reprojection output.\")\n",
    "\n",
    "        profile = src.profile.copy()\n",
    "        profile.update(\n",
    "            crs=target_crs,\n",
    "            transform=transform,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            nodata=nodata,\n",
    "        )\n",
    "\n",
    "        if dry_run:\n",
    "            log(f\"[DRY RUN] Would reproject: {in_path} -> {out_path}\")\n",
    "            return out_path\n",
    "\n",
    "        with timed_step(f\"Reproject {layer_name}\"):\n",
    "            with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "                for i in range(1, src.count + 1):\n",
    "                    reproject(\n",
    "                        source=rasterio.band(src, i),\n",
    "                        destination=rasterio.band(dst, i),\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=target_crs,\n",
    "                        src_nodata=src.nodata,\n",
    "                        dst_nodata=nodata,\n",
    "                        resampling=Resampling.bilinear,\n",
    "                    )\n",
    "\n",
    "        log(f\"Wrote: {out_path}\")\n",
    "        return out_path\n",
    "\n",
    "def summarize_raster(path: Path, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get summary stats for a raster file.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    stats = summarize_raster(OUT_PREP[\"dem_mosaic\"], \"dem\")\n",
    "    \"\"\"\n",
    "    with rasterio.open(path) as src:\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"path\": path.name,\n",
    "            \"crs\": str(src.crs),\n",
    "            \"dimensions\": f\"{src.width} x {src.height}\",\n",
    "            \"resolution\": f\"{src.res[0]:.2f} x {src.res[1]:.2f}\",\n",
    "            \"dtype\": src.dtypes[0],\n",
    "            \"bands\": src.count,\n",
    "            \"size_mb\": path.stat().st_size / (1024 * 1024),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58109d10",
   "metadata": {},
   "source": [
    "#### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75613d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_tmk(tmk) -> bool:\n",
    "    \"\"\"\n",
    "    Check if TMK is valid: 9 digits, numeric only, not null.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    valid_mask = parcels['tmk'].apply(is_valid_tmk)\n",
    "    \"\"\"\n",
    "    if pd.isna(tmk):\n",
    "        return False\n",
    "    s = str(tmk).strip()\n",
    "    return len(s) == 9 and s.isdigit()\n",
    "\n",
    "def ensure_tmk_string(gdf, tmk_col: str, layer_name: str):\n",
    "    \"\"\"\n",
    "    Ensure TMK column is string type (9 digits, zero-padded).\n",
    "    \n",
    "    Handles numeric TMKs that lost leading zeros by converting through Int64.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame\n",
    "        Input GeoDataFrame.\n",
    "    tmk_col : str\n",
    "        Name of TMK column.\n",
    "    layer_name : str\n",
    "        Name for logging.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        GeoDataFrame with TMK as string type.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    parcels = ensure_tmk_string(parcels, \"tmk_parcels\", \"parcels\")\n",
    "    \"\"\"\n",
    "    if gdf[tmk_col].dtype == \"string\":\n",
    "        log(f\"{layer_name}: {tmk_col} already string type\")\n",
    "        return gdf\n",
    "    \n",
    "    log(f\"{layer_name}: converting {tmk_col} to string (was {gdf[tmk_col].dtype})\")\n",
    "    gdf[tmk_col] = (\n",
    "        pd.to_numeric(gdf[tmk_col], errors=\"coerce\")\n",
    "        .round(0)\n",
    "        .astype(\"Int64\")\n",
    "        .astype(\"string\")\n",
    "        .str.zfill(9)  # Zero-pad to 9 digits\n",
    "    )\n",
    "    return gdf\n",
    "\n",
    "def load_and_prep_cesspools(\n",
    "    in_path: Path,\n",
    "    keep_cols: list,\n",
    "    rename_map: dict,\n",
    "    tmk_col: str,\n",
    "    filter_col: str,\n",
    "    filter_value,\n",
    "    target_crs: str,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load cesspool layer and apply standard prep: subset, rename, reproject, \n",
    "    validate geometry, ensure TMK is string, filter to class IV.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    cesspools = load_and_prep_cesspools(\n",
    "        in_path=SOURCE / \"cesspools.shp\",\n",
    "        keep_cols=CONFIG[\"fields\"][\"cesspools\"][\"keep_cols\"],\n",
    "        rename_map=CONFIG[\"fields\"][\"cesspools\"][\"rename_map\"],\n",
    "        tmk_col=\"tmk_cps\",\n",
    "        filter_col=\"class_iv\",\n",
    "        filter_value=0,\n",
    "        target_crs=TARGET_CRS,\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Load\n",
    "    cesspools = read_vector(in_path, \"cesspools_raw\")\n",
    "    \n",
    "    # Subset columns\n",
    "    cols_with_geom = keep_cols + [\"geometry\"]\n",
    "    log(f\"Subsetting to columns: {cols_with_geom}\")\n",
    "    cesspools = cesspools[cols_with_geom].copy()\n",
    "    \n",
    "    # Rename columns\n",
    "    log(f\"Renaming columns: {rename_map}\")\n",
    "    cesspools = cesspools.rename(columns=rename_map)\n",
    "    \n",
    "    # Reproject\n",
    "    cesspools = ensure_vector_crs(cesspools, target_crs, \"cesspools\")\n",
    "    \n",
    "    # Validate geometries\n",
    "    cesspools = validate_geometry_if_needed(cesspools, \"cesspools\")\n",
    "    \n",
    "    # Remove null/empty geometries\n",
    "    n_before = len(cesspools)\n",
    "    null_mask = cesspools.geometry.isna()\n",
    "    empty_mask = cesspools.geometry.is_empty\n",
    "    cesspools = cesspools[~null_mask & ~empty_mask].copy()\n",
    "    n_removed = n_before - len(cesspools)\n",
    "    if n_removed > 0:\n",
    "        log(f\"Removed {n_removed:,} cesspools with null/empty geometries\")\n",
    "    \n",
    "    # Ensure TMK is string\n",
    "    cesspools = ensure_tmk_string(cesspools, tmk_col, \"cesspools\")\n",
    "    \n",
    "    # Filter to class IV (filter_col != filter_value)\n",
    "    n_before = len(cesspools)\n",
    "    cesspools = cesspools[cesspools[filter_col] != filter_value].copy()\n",
    "    n_filtered = n_before - len(cesspools)\n",
    "    log(f\"Filtered to {filter_col} != {filter_value}: {len(cesspools):,} rows ({n_filtered:,} removed)\")\n",
    "    \n",
    "    return cesspools\n",
    "\n",
    "\n",
    "def check_tmk_correction_preconditions(\n",
    "    cesspools: gpd.GeoDataFrame,\n",
    "    parcels: gpd.GeoDataFrame,\n",
    "    cp_tmk_col: str,\n",
    "    pr_tmk_col: str,\n",
    "    target_crs: str,\n",
    "    thresh_m: float,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Verify inputs before TMK correction workflow. Raises errors if preconditions fail.\n",
    "    Returns count of malformed cesspool TMKs (for logging).\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    n_malformed = check_tmk_correction_preconditions(\n",
    "        cesspools, parcels, \"tmk_cps\", \"tmk_parcels\", TARGET_CRS, 25\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Ensure required columns exist\n",
    "    required_cp = [cp_tmk_col, \"geometry\"]\n",
    "    required_pr = [pr_tmk_col, \"geometry\"]\n",
    "    \n",
    "    missing_cp = [c for c in required_cp if c not in cesspools.columns]\n",
    "    missing_pr = [c for c in required_pr if c not in parcels.columns]\n",
    "    \n",
    "    if missing_cp:\n",
    "        raise KeyError(f\"cesspools missing required columns: {missing_cp}\")\n",
    "    if missing_pr:\n",
    "        raise KeyError(f\"parcels missing required columns: {missing_pr}\")\n",
    "    \n",
    "    # Ensure CRS match\n",
    "    if cesspools.crs is None:\n",
    "        raise ValueError(\"cesspools has no CRS set.\")\n",
    "    if parcels.crs is None:\n",
    "        raise ValueError(\"parcels has no CRS set.\")\n",
    "    if cesspools.crs != parcels.crs:\n",
    "        raise ValueError(f\"CRS mismatch: cesspools={cesspools.crs}, parcels={parcels.crs}\")\n",
    "    \n",
    "    # Ensure no null/empty geometries\n",
    "    if cesspools.geometry.isna().any() or cesspools.geometry.is_empty.any():\n",
    "        raise ValueError(\"cesspools contains null/empty geometries.\")\n",
    "    if parcels.geometry.isna().any() or parcels.geometry.is_empty.any():\n",
    "        raise ValueError(\"parcels contains null/empty geometries.\")\n",
    "    \n",
    "    # Parcels TMK quality (authoritative - should be clean)\n",
    "    pr_invalid = ~parcels[pr_tmk_col].apply(is_valid_tmk)\n",
    "    if pr_invalid.any():\n",
    "        raise ValueError(\n",
    "            f\"Parcels (authoritative) has {pr_invalid.sum():,} invalid TMKs. Fix upstream.\"\n",
    "        )\n",
    "    \n",
    "    # Cesspools TMK quality (will be corrected)\n",
    "    cp_malformed_count = int((~cesspools[cp_tmk_col].apply(is_valid_tmk)).sum())\n",
    "    \n",
    "    log(\"Preconditions passed âœ“\")\n",
    "    log(f\"  Cesspools: {len(cesspools):,} | Parcels: {len(parcels):,}\")\n",
    "    log(f\"  CRS: {target_crs} | Threshold: {thresh_m}m\")\n",
    "    log(f\"  Malformed cesspool TMKs: {cp_malformed_count:,} (will be flagged)\")\n",
    "    \n",
    "    return cp_malformed_count\n",
    "\n",
    "def prep_vector(\n",
    "    in_path: Path,\n",
    "    out_path: Path,\n",
    "    layer_name: str,\n",
    "    target_crs: str,\n",
    "    keep_cols: list[str] | None = None,\n",
    "    rename_map: dict[str, str] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prep a vector dataset to the project target CRS and write to a single-layer GPKG.\n",
    "\n",
    "    Workflow:\n",
    "      read -> validate geometry -> reproject if needed -> optional subset/rename -> QA -> write gpkg\n",
    "    \"\"\"\n",
    "    gdf = read_vector(in_path, layer_name)\n",
    "\n",
    "    gdf = validate_geometry_if_needed(gdf, layer_name)\n",
    "    gdf = ensure_vector_crs(gdf, target_crs, layer_name)\n",
    "\n",
    "    if keep_cols is not None:\n",
    "        keep_cols = keep_cols + [\"geometry\"]\n",
    "        missing = [c for c in keep_cols if c not in gdf.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f\"{layer_name}: missing required columns: {missing}\")\n",
    "\n",
    "        log(f\"{layer_name}: subsetting to columns: {keep_cols}\")\n",
    "        gdf = gdf[keep_cols].copy()\n",
    "\n",
    "    if rename_map:\n",
    "        log(f\"{layer_name}: renaming columns: {rename_map}\")\n",
    "        gdf = gdf.rename(columns=rename_map)\n",
    "\n",
    "    _ = check_geometry_quality(gdf, layer_name)\n",
    "\n",
    "    write_vector_gpkg(gdf, out_path, layer_name)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814bc13a",
   "metadata": {},
   "source": [
    "#### QA/QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071a465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_island_column(gdf, tmk_col: str, out_col: str = \"island\"):\n",
    "    \"\"\"\n",
    "    Add island name derived from the first TMK digit.\n",
    "\n",
    "    TMK convention used here:\n",
    "      1 = Hawaii, 2 = Maui, 3 = Oahu, 4 = Kauai\n",
    "    \"\"\"\n",
    "    island_map = {\"1\": \"Hawaii\", \"2\": \"Maui\", \"3\": \"Oahu\", \"4\": \"Kauai\"}\n",
    "\n",
    "    if gdf is None or len(gdf) == 0:\n",
    "        return gdf\n",
    "\n",
    "    if tmk_col not in gdf.columns:\n",
    "        gdf[out_col] = \"Unknown\"\n",
    "        return gdf\n",
    "\n",
    "    gdf[out_col] = gdf[tmk_col].astype(\"string\").str[0].map(island_map).fillna(\"Unknown\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def _style(fill_color: str, line_color: str, weight: int = 2, fill_opacity: float = 0.3):\n",
    "    \"\"\"\n",
    "    Folium style function factory (avoids lambda closure issues).\n",
    "    \"\"\"\n",
    "    def fn(_):\n",
    "        return {\n",
    "            \"fillColor\": fill_color,\n",
    "            \"color\": line_color,\n",
    "            \"weight\": weight,\n",
    "            \"fillOpacity\": fill_opacity,\n",
    "        }\n",
    "    return fn\n",
    "\n",
    "\n",
    "def build_review_qaqc_map(\n",
    "    review_main: gpd.GeoDataFrame,\n",
    "    parcels: gpd.GeoDataFrame,\n",
    "    web_crs: str,\n",
    "    cp_tmk_col: str,\n",
    "    pr_tmk_col: str,\n",
    "    nearest_pr_tmk_col: str = \"nearest_tmk_parcels\",\n",
    "    status_col: str = \"tmk_status\",\n",
    "    last_digit_col: str = \"last_digit_off_by_1\",\n",
    "    dist_nearest_col: str = \"dist_to_nearest_m\",\n",
    "    dist_claimed_col: str = \"dist_to_tmk_cps_parcel_m\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build an interactive Folium QA/QC map for review cases.\n",
    "\n",
    "    Expects review_main to contain only review rows (not the full corrected layer),\n",
    "    but will still work if review_main includes other statuses.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    folium.Map\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # Preconditions\n",
    "    # -----------------------------\n",
    "    required = [status_col, cp_tmk_col, \"geometry\"]\n",
    "    missing = [c for c in required if c not in review_main.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"review_main missing required columns: {missing}\")\n",
    "\n",
    "    if review_main.empty:\n",
    "        log(\"QA/QC map: review_main is empty; nothing to map.\")\n",
    "        return None\n",
    "\n",
    "    # Work on copies (do not mutate upstream objects)\n",
    "    review = review_main.copy()\n",
    "    pr = parcels[[pr_tmk_col, \"geometry\"]].copy()\n",
    "\n",
    "    # Ensure WEB CRS for mapping\n",
    "    review_web = review.to_crs(web_crs)\n",
    "    pr_web = pr.to_crs(web_crs)\n",
    "\n",
    "    # Normalize last_digit_off_by_1 to bool when present\n",
    "    if last_digit_col in review_web.columns:\n",
    "        # Accept bool, 0/1, \"0\"/\"1\", NaN\n",
    "        review_web[last_digit_col] = (\n",
    "            pd.to_numeric(review_web[last_digit_col], errors=\"coerce\")\n",
    "            .fillna(0)\n",
    "            .astype(int)\n",
    "            .astype(bool)\n",
    "        )\n",
    "\n",
    "    # Island label (non-blocking if TMK malformed/NA)\n",
    "    island_map = {\"1\": \"Hawaii\", \"2\": \"Maui\", \"3\": \"Oahu\", \"4\": \"Kauai\"}\n",
    "    review_web[\"island\"] = (\n",
    "        review_web[cp_tmk_col].astype(\"string\").str[0].map(island_map).fillna(\"Unknown\")\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Split review subsets\n",
    "    # -----------------------------\n",
    "    review_hit = review_web.loc[review_web[status_col] == \"hit_but_tmk_exists_elsewhere\"].copy()\n",
    "    review_nohit = review_web.loc[review_web[status_col] == \"review_nohit_beyond_threshold\"].copy()\n",
    "    review_malformed = review_web.loc[review_web[status_col] == \"tmk_malformed\"].copy()\n",
    "\n",
    "    if len(review_nohit) > 0 and last_digit_col in review_nohit.columns:\n",
    "        review_nohit_off_by_1 = review_nohit.loc[review_nohit[last_digit_col] == True].copy()\n",
    "        review_nohit_way_off = review_nohit.loc[review_nohit[last_digit_col] == False].copy()\n",
    "    else:\n",
    "        review_nohit_off_by_1 = gpd.GeoDataFrame(columns=review_web.columns, crs=review_web.crs)\n",
    "        review_nohit_way_off = gpd.GeoDataFrame(columns=review_web.columns, crs=review_web.crs)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Map extent\n",
    "    # -----------------------------\n",
    "    all_bounds = review_web.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    center_lat = (all_bounds[1] + all_bounds[3]) / 2\n",
    "    center_lon = (all_bounds[0] + all_bounds[2]) / 2\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n",
    "    m.fit_bounds([[all_bounds[1], all_bounds[0]], [all_bounds[3], all_bounds[2]]])\n",
    "\n",
    "    # -----------------------------\n",
    "    # HIT AMBIGUOUS\n",
    "    # -----------------------------\n",
    "    if len(review_hit) > 0:\n",
    "        fg_hit_parcel_a = folium.FeatureGroup(name=\"[HIT AMBIGUOUS] Parcel A: where point landed (purple)\")\n",
    "        fg_hit_parcel_b = folium.FeatureGroup(name=\"[HIT AMBIGUOUS] Parcel B: claimed by tmk_cps (blue)\")\n",
    "        fg_hit_points = folium.FeatureGroup(name=\"[HIT AMBIGUOUS] Cesspool points (blue)\")\n",
    "\n",
    "        hit_parcel_a_tmks = review_hit[pr_tmk_col].dropna().unique()\n",
    "        hit_parcel_b_tmks = review_hit[cp_tmk_col].dropna().unique()\n",
    "\n",
    "        hit_parcels_a = pr_web.loc[pr_web[pr_tmk_col].isin(hit_parcel_a_tmks)].copy()\n",
    "        hit_parcels_b = pr_web.loc[pr_web[pr_tmk_col].isin(hit_parcel_b_tmks)].copy()\n",
    "\n",
    "        for _, row in hit_parcels_a.iterrows():\n",
    "            folium.GeoJson(\n",
    "                row.geometry.__geo_interface__,\n",
    "                style_function=lambda x: {\"fillColor\": \"purple\", \"color\": \"purple\", \"weight\": 2, \"fillOpacity\": 0.3},\n",
    "                tooltip=f\"Parcel A (landed): {row[pr_tmk_col]}\",\n",
    "            ).add_to(fg_hit_parcel_a)\n",
    "\n",
    "        for _, row in hit_parcels_b.iterrows():\n",
    "            folium.GeoJson(\n",
    "                row.geometry.__geo_interface__,\n",
    "                style_function=lambda x: {\"fillColor\": \"blue\", \"color\": \"blue\", \"weight\": 2, \"fillOpacity\": 0.3},\n",
    "                tooltip=f\"Parcel B (claimed): {row[pr_tmk_col]}\",\n",
    "            ).add_to(fg_hit_parcel_b)\n",
    "\n",
    "        for _, row in review_hit.iterrows():\n",
    "            tooltip_text = (\n",
    "                f\"tmk_cps (claimed): {row.get(cp_tmk_col, 'NA')}<br>\"\n",
    "                f\"tmk_parcels (landed): {row.get(pr_tmk_col, 'NA')}<br>\"\n",
    "                f\"Dist to claimed parcel: {row.get(dist_claimed_col, 'N/A')} m<br>\"\n",
    "                f\"Island: {row.get('island', 'Unknown')}\"\n",
    "            )\n",
    "            folium.CircleMarker(\n",
    "                location=[row.geometry.y, row.geometry.x],\n",
    "                radius=4,\n",
    "                color=\"blue\",\n",
    "                fill=True,\n",
    "                fillColor=\"blue\",\n",
    "                fillOpacity=0.8,\n",
    "                tooltip=tooltip_text,\n",
    "            ).add_to(fg_hit_points)\n",
    "\n",
    "        fg_hit_parcel_a.add_to(m)\n",
    "        fg_hit_parcel_b.add_to(m)\n",
    "        fg_hit_points.add_to(m)\n",
    "\n",
    "    # -----------------------------\n",
    "    # NO-HIT OFF BY 1 (orange)\n",
    "    # -----------------------------\n",
    "    if len(review_nohit_off_by_1) > 0:\n",
    "        fg_nohit_off1_parcel = folium.FeatureGroup(name=\"[NO-HIT OFF BY 1] Nearest parcel (orange)\")\n",
    "        fg_nohit_off1_points = folium.FeatureGroup(name=\"[NO-HIT OFF BY 1] Cesspool points (orange)\")\n",
    "\n",
    "        if nearest_pr_tmk_col not in review_nohit_off_by_1.columns:\n",
    "            log(f\"WARNING: {nearest_pr_tmk_col} missing; cannot draw nearest parcels for no-hit.\")\n",
    "        else:\n",
    "            nohit_off1_tmks = review_nohit_off_by_1[nearest_pr_tmk_col].dropna().unique()\n",
    "            nohit_off1_parcels = pr_web.loc[pr_web[pr_tmk_col].isin(nohit_off1_tmks)].copy()\n",
    "\n",
    "            for _, row in nohit_off1_parcels.iterrows():\n",
    "                folium.GeoJson(\n",
    "                    row.geometry.__geo_interface__,\n",
    "                    style_function=lambda x: {\"fillColor\": \"orange\", \"color\": \"orange\", \"weight\": 2, \"fillOpacity\": 0.3},\n",
    "                    tooltip=f\"Nearest parcel: {row[pr_tmk_col]}\",\n",
    "                ).add_to(fg_nohit_off1_parcel)\n",
    "\n",
    "        for _, row in review_nohit_off_by_1.iterrows():\n",
    "            tooltip_text = (\n",
    "                f\"tmk_cps: {row.get(cp_tmk_col, 'NA')}<br>\"\n",
    "                f\"Nearest parcel: {row.get(nearest_pr_tmk_col, 'NA')}<br>\"\n",
    "                f\"Dist to nearest: {row.get(dist_nearest_col, 'N/A')} m<br>\"\n",
    "                f\"Last digit off by 1: True<br>\"\n",
    "                f\"Island: {row.get('island', 'Unknown')}\"\n",
    "            )\n",
    "            folium.CircleMarker(\n",
    "                location=[row.geometry.y, row.geometry.x],\n",
    "                radius=4,\n",
    "                color=\"orange\",\n",
    "                fill=True,\n",
    "                fillColor=\"orange\",\n",
    "                fillOpacity=0.8,\n",
    "                tooltip=tooltip_text,\n",
    "            ).add_to(fg_nohit_off1_points)\n",
    "\n",
    "        fg_nohit_off1_parcel.add_to(m)\n",
    "        fg_nohit_off1_points.add_to(m)\n",
    "\n",
    "    # -----------------------------\n",
    "    # NO-HIT WAY OFF (red)\n",
    "    # -----------------------------\n",
    "    if len(review_nohit_way_off) > 0:\n",
    "        fg_nohit_wayoff_parcel = folium.FeatureGroup(name=\"[NO-HIT WAY OFF] Nearest parcel (red)\")\n",
    "        fg_nohit_wayoff_points = folium.FeatureGroup(name=\"[NO-HIT WAY OFF] Cesspool points - MANUAL REVIEW (red)\")\n",
    "\n",
    "        if nearest_pr_tmk_col not in review_nohit_way_off.columns:\n",
    "            log(f\"WARNING: {nearest_pr_tmk_col} missing; cannot draw nearest parcels for no-hit.\")\n",
    "        else:\n",
    "            nohit_wayoff_tmks = review_nohit_way_off[nearest_pr_tmk_col].dropna().unique()\n",
    "            nohit_wayoff_parcels = pr_web.loc[pr_web[pr_tmk_col].isin(nohit_wayoff_tmks)].copy()\n",
    "\n",
    "            for _, row in nohit_wayoff_parcels.iterrows():\n",
    "                folium.GeoJson(\n",
    "                    row.geometry.__geo_interface__,\n",
    "                    style_function=lambda x: {\"fillColor\": \"red\", \"color\": \"red\", \"weight\": 2, \"fillOpacity\": 0.3},\n",
    "                    tooltip=f\"Nearest parcel: {row[pr_tmk_col]}\",\n",
    "                ).add_to(fg_nohit_wayoff_parcel)\n",
    "\n",
    "        for _, row in review_nohit_way_off.iterrows():\n",
    "            tooltip_text = (\n",
    "                f\"tmk_cps: {row.get(cp_tmk_col, 'NA')}<br>\"\n",
    "                f\"Nearest parcel: {row.get(nearest_pr_tmk_col, 'NA')}<br>\"\n",
    "                f\"Dist to nearest: {row.get(dist_nearest_col, 'N/A')} m<br>\"\n",
    "                f\"Last digit off by 1: False<br>\"\n",
    "                f\"Island: {row.get('island', 'Unknown')}<br>\"\n",
    "                f\"<b>NEEDS MANUAL REVIEW</b>\"\n",
    "            )\n",
    "            folium.CircleMarker(\n",
    "                location=[row.geometry.y, row.geometry.x],\n",
    "                radius=4,\n",
    "                color=\"red\",\n",
    "                fill=True,\n",
    "                fillColor=\"red\",\n",
    "                fillOpacity=0.8,\n",
    "                tooltip=tooltip_text,\n",
    "            ).add_to(fg_nohit_wayoff_points)\n",
    "\n",
    "        fg_nohit_wayoff_parcel.add_to(m)\n",
    "        fg_nohit_wayoff_points.add_to(m)\n",
    "\n",
    "    # -----------------------------\n",
    "    # TMK MALFORMED (yellow)\n",
    "    # -----------------------------\n",
    "    if len(review_malformed) > 0:\n",
    "        fg_malformed_points = folium.FeatureGroup(name=\"[TMK MALFORMED] Cesspool points (yellow)\")\n",
    "\n",
    "        for _, row in review_malformed.iterrows():\n",
    "            tooltip_text = (\n",
    "                f\"tmk_cps: {row.get(cp_tmk_col, 'NA')}<br>\"\n",
    "                f\"Island: {row.get('island', 'Unknown')}<br>\"\n",
    "                f\"<b>MALFORMED TMK - NEEDS MANUAL REVIEW</b>\"\n",
    "            )\n",
    "            folium.CircleMarker(\n",
    "                location=[row.geometry.y, row.geometry.x],\n",
    "                radius=4,\n",
    "                color=\"yellow\",\n",
    "                fill=True,\n",
    "                fillColor=\"yellow\",\n",
    "                fillOpacity=0.8,\n",
    "                tooltip=tooltip_text,\n",
    "            ).add_to(fg_malformed_points)\n",
    "\n",
    "        fg_malformed_points.add_to(m)\n",
    "\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Legend (counts reflect what was actually plotted)\n",
    "    # -----------------------------\n",
    "    log(\"QA/QC REVIEW MAP LEGEND\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n[HIT AMBIGUOUS] hit_but_tmk_exists_elsewhere:\")\n",
    "    print(\"  Purple polygon = Parcel A (where point landed)\")\n",
    "    print(\"  Blue polygon   = Parcel B (claimed by tmk_cps)\")\n",
    "    print(\"  Blue point     = Cesspool\")\n",
    "    print(f\"  Count: {len(review_hit):,}\")\n",
    "\n",
    "    print(\"\\n[NO-HIT OFF BY 1] last_digit_off_by_1=True:\")\n",
    "    print(\"  Orange polygon = Nearest parcel\")\n",
    "    print(\"  Orange point   = Cesspool (likely typo)\")\n",
    "    print(f\"  Count: {len(review_nohit_off_by_1):,}\")\n",
    "\n",
    "    print(\"\\n[NO-HIT WAY OFF] last_digit_off_by_1=False:\")\n",
    "    print(\"  Red polygon    = Nearest parcel\")\n",
    "    print(\"  Red point      = Cesspool (NEEDS MANUAL REVIEW)\")\n",
    "    print(f\"  Count: {len(review_nohit_way_off):,}\")\n",
    "\n",
    "    if len(review_malformed) > 0:\n",
    "        print(\"\\n[TMK MALFORMED]:\")\n",
    "        print(\"  Yellow point   = Cesspool (invalid TMK format)\")\n",
    "        print(f\"  Count: {len(review_malformed):,}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Total review cases: {len(review_web):,}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ee54b",
   "metadata": {},
   "source": [
    "## Geoprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797682ec",
   "metadata": {},
   "source": [
    "### Step 1: Preprocess Parcels\n",
    "\n",
    "- Input: `data/02_interim/parcels_hi_higp_repaired_32604/tmk_state.shp`\n",
    "- Output: `data/02_interim/parcels_hi_higp_validated_32604.gpkg`\n",
    "- Workflow: \n",
    "    - Load raw parcels polygon (python)\n",
    "    - Load and repair raw parcels polygon using Repair Geometry (ArcGIS; manual step)\n",
    "    - Reload repaired parcels polygon (python)\n",
    "    - Reproject to 32604 (if needed)\n",
    "    - Validate remaining invalid geometries using Shapely (`make_valid`)\n",
    "    - Identify parcel rows for TMK `139019004`\n",
    "    - If duplicate parcel geometries exist:\n",
    "        - Drop null or empty geometries\n",
    "        - Validate geometries if needed\n",
    "        - Dissolve parcel parts into a single geometry using pairwise Shapely `geometry.union()`\n",
    "            - (avoids `union_all` / `unary_union`, which fail in this environment)\n",
    "        - Replace the multiple parcel rows with the single merged parcel geometry\n",
    "    - QA: assert TMK `139019004` appears exactly once in `parcels`\n",
    "    - Export as GPKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b209ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load error in python\n",
    "# parcels_path = SOURCE / \"parcels_hi_higp/tmk_state.shp\"\n",
    "# parcels = gpd.read_file(parcels_path)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# GEOSException         Traceback (most recent call last)\n",
    "# Cell In[3], line 2\n",
    "#       1 filepath = RAW / \"parcels_hi_higp/tmk_state.shp\"\n",
    "# ----> 2 gdf = gpd.read_file(filepath)\n",
    "\n",
    "# File ~\\miniconda3\\envs\\geo\\Lib\\site-packages\\geopandas\\io\\file.py:294, in _read_file(filename, bbox, mask, columns, rows, engine, **kwargs)\n",
    "#     291             from_bytes = True\n",
    "#     293 if engine == \"pyogrio\":\n",
    "# --> 294     return _read_file_pyogrio(\n",
    "#     295         filename, bbox=bbox, mask=mask, columns=columns, rows=rows, **kwargs\n",
    "#     296     )\n",
    "#     298 elif engine == \"fiona\":\n",
    "#     299     if pd.api.types.is_file_like(filename):\n",
    "\n",
    "# File ~\\miniconda3\\envs\\geo\\Lib\\site-packages\\geopandas\\io\\file.py:547, in _read_file_pyogrio(path_or_bytes, bbox, mask, rows, **kwargs)\n",
    "#     538     warnings.warn(\n",
    "#     539         \"The 'include_fields' and 'ignore_fields' keywords are deprecated, and \"\n",
    "#     540         \"will be removed in a future release. You can use the 'columns' keyword \"\n",
    "#    (...)\n",
    "#     543         stacklevel=3,\n",
    "#     544     )\n",
    "#     545     kwargs[\"columns\"] = kwargs.pop(\"include_fields\")\n",
    "# --> 547 return pyogrio.read_dataframe(path_or_bytes, bbox=bbox, **kwargs)\n",
    "\n",
    "# File ~\\miniconda3\\envs\\geo\\Lib\\site-packages\\pyogrio\\geopandas.py:327, in read_dataframe(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\n",
    "#     324 if geometry is None or not read_geometry:\n",
    "#     325     return df\n",
    "# --> 327 geometry = shapely.from_wkb(geometry, on_invalid=on_invalid)\n",
    "#     329 return gp.GeoDataFrame(df, geometry=geometry, crs=meta[\"crs\"])\n",
    "\n",
    "# File ~\\miniconda3\\envs\\geo\\Lib\\site-packages\\shapely\\io.py:320, in from_wkb(geometry, on_invalid, **kwargs)\n",
    "#     316 # ensure the input has object dtype, to avoid numpy inferring it as a\n",
    "#     317 # fixed-length string dtype (which removes trailing null bytes upon access\n",
    "#     318 # of array elements)\n",
    "#     319 geometry = np.asarray(geometry, dtype=object)\n",
    "# --> 320 return lib.from_wkb(geometry, invalid_handler, **kwargs)\n",
    "\n",
    "# GEOSException: IllegalArgumentException: Points of LinearRing do not form a closed linestring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd0be4",
   "metadata": {},
   "source": [
    "The parcel layer geometry was repaired in ArcGIS Pro using the Repair Geometry tool with default settings to correct invalid polygon rings and topology errors. The repaired output was saved as an intermediate layer at `data/02_interim/parcels_hi_higp_repaired_32604/tmk_state.shp` and used as input for subsequent geometry validation steps using Shapely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Step 1: Prep parcels (repaired > preprocessed)\")\n",
    "\n",
    "in_path = IN_INTERIM[\"parcels_repaired\"]\n",
    "out_path = OUT_PREP[\"parcels_preprocessed\"]\n",
    "\n",
    "parcels_cfg = CONFIG[\"fields\"][\"parcels\"]\n",
    "keep_cols = parcels_cfg[\"keep_cols\"] + [\"geometry\"]\n",
    "rename_map = parcels_cfg[\"rename_map\"]\n",
    "tmk_col = parcels_cfg[\"tmk_col\"]\n",
    "target_dup_tmk = parcels_cfg[\"target_dup_tmk\"]\n",
    "\n",
    "log(f\"Input:  {in_path}\")\n",
    "log(f\"Output: {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; loading from interim output.\")\n",
    "    parcels = read_vector(out_path, \"parcels_preprocessed\")\n",
    "    _ = check_geometry_quality(parcels, \"parcels\")\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        parcels = read_vector(in_path, \"parcels_repaired\")\n",
    "\n",
    "        parcels = validate_geometry_if_needed(parcels, \"parcels\")\n",
    "        parcels = ensure_vector_crs(parcels, TARGET_CRS, \"parcels\")\n",
    "\n",
    "        missing_cols = [c for c in keep_cols if c not in parcels.columns]\n",
    "        if missing_cols:\n",
    "            raise KeyError(f\"Missing required columns in parcels input: {missing_cols}\")\n",
    "\n",
    "        log(f\"Subsetting to columns: {keep_cols}\")\n",
    "        parcels = parcels[keep_cols].copy()\n",
    "\n",
    "        log(f\"Renaming columns: {rename_map}\")\n",
    "        parcels = parcels.rename(columns=rename_map)\n",
    "\n",
    "        parcels = ensure_tmk_string(parcels, tmk_col, \"parcels\")\n",
    "\n",
    "        valid_tmk_mask = parcels[tmk_col].apply(is_valid_tmk)\n",
    "        n_malformed = int((~valid_tmk_mask).sum())\n",
    "        if n_malformed > 0:\n",
    "            malformed_values = parcels.loc[~valid_tmk_mask, tmk_col].astype(\"string\").unique().tolist()\n",
    "            log(f\"Filtering {n_malformed:,} parcels with malformed TMKs: {malformed_values}\")\n",
    "            parcels = parcels[valid_tmk_mask].copy()\n",
    "        else:\n",
    "            log(\"TMK QA: all TMKs valid (9 numeric digits)\")\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Targeted duplicate attribute consistency check (pre-dissolve)\n",
    "        # ------------------------------------------------------------\n",
    "        target_tmk_str = str(target_dup_tmk)\n",
    "        target_mask = (parcels[tmk_col].astype(\"string\") == target_tmk_str)\n",
    "        target_n = int(target_mask.sum())\n",
    "\n",
    "        if target_n > 1:\n",
    "            dup_parts = parcels.loc[target_mask].copy()\n",
    "            non_geom_cols = [c for c in dup_parts.columns if c != \"geometry\"]\n",
    "\n",
    "            inconsistent = [\n",
    "                c for c in non_geom_cols\n",
    "                if dup_parts[c].astype(\"string\").nunique(dropna=False) > 1\n",
    "            ]\n",
    "\n",
    "            if inconsistent:\n",
    "                log(\n",
    "                    f\"WARNING parcels: target TMK {target_tmk_str} has inconsistent \"\n",
    "                    f\"attributes across duplicates: {inconsistent}\"\n",
    "                )\n",
    "        # ------------------------------------------------------------\n",
    "\n",
    "        # Targeted dissolve as part of the prep step\n",
    "        parcels = dissolve_duplicate_tmk_polygons(\n",
    "            parcels=parcels,\n",
    "            tmk_col=tmk_col,\n",
    "            target_tmk=target_dup_tmk,\n",
    "            layer_name=\"parcels\",\n",
    "        )\n",
    "\n",
    "        _ = check_geometry_quality(parcels, \"parcels\")\n",
    "        log(f\"Final columns: {parcels.columns.tolist()}\")\n",
    "\n",
    "        if DRY_RUN:\n",
    "            log(f\"[DRY RUN] Would write: {out_path}\")\n",
    "        else:\n",
    "            write_vector_gpkg(parcels, out_path, \"parcels_preprocessed\")\n",
    "\n",
    "        del valid_tmk_mask, n_malformed\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 1: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise\n",
    "\n",
    "log(f\"Step 1 complete: {len(parcels):,} parcels ready for cesspool TMK correction\")\n",
    "display(parcels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02587d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick map check: confirm TMK 139019004 is now a single parcel row + visualize window\n",
    "log(\"Map check: verify parcel dissolve for TMK 139019004\")\n",
    "\n",
    "base_tmk = \"139019004\"\n",
    "\n",
    "# Build small TMK window for spatial context\n",
    "base_int = int(base_tmk)\n",
    "tmk_window = [str(base_int + i) for i in range(-2, 3)]\n",
    "\n",
    "# Slice parcels + cesspools using existing objects\n",
    "par_near = parcels.loc[\n",
    "    parcels[\"tmk_parcels\"].astype(\"string\").isin(tmk_window)\n",
    "].copy()\n",
    "\n",
    "# cp_near = cesspools.loc[\n",
    "#     cesspools[\"tmk_validated\"].astype(\"string\").isin(tmk_window)\n",
    "# ].copy()\n",
    "\n",
    "# QA: confirm exactly one parcel row for target TMK\n",
    "n_target = int((par_near[\"tmk_parcels\"] == base_tmk).sum())\n",
    "log(f\"Parcels rows for target TMK in window: {n_target} (expected 1)\")\n",
    "\n",
    "# Parcel map\n",
    "m = par_near.explore(\n",
    "    column=\"tmk_parcels\",\n",
    "    categorical=True,\n",
    "    legend=True,\n",
    "    style_kwds={\n",
    "        \"fillOpacity\": 0.25,\n",
    "        \"weight\": 2,\n",
    "    },\n",
    ")\n",
    "\n",
    "# # Overlay cesspools\n",
    "# cp_near.explore(\n",
    "#     m=m,\n",
    "#     color=\"blue\",\n",
    "#     marker_kwds={\"radius\": 5},\n",
    "#     tooltip=[\n",
    "#         \"tmk_validated\",\n",
    "#         \"class_iv\",\n",
    "#         \"effluent\",\n",
    "#         \"nitrogen\",\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0eecf",
   "metadata": {},
   "source": [
    "### Step 2: Prep rasters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3ae0b",
   "metadata": {},
   "source": [
    "- Workflow: \n",
    "    - Load raw raster\n",
    "    - Mosaic tiles (if needed)\n",
    "    - Reproject to 32604 (if needed)\n",
    "    - Export GeoTIFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd4945",
   "metadata": {},
   "source": [
    "#### Step 2a: DEM mosaic\n",
    "\n",
    "- Inputs: `data/01_inputs/source/dem_hi_pacioos/*.tif`\n",
    "- Output: `data/01_inputs/prepared/dem_hi_pacioos_mosaic_32604.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb48ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2a: Mosaic DEM rasters ---\n",
      "\n",
      "--- Input dir: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\dem_hi_pacioos ---\n",
      "\n",
      "--- Output:    f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\dem_hi_pacioos_mosaic_32604\\dem_hi_pacioos_mosaic_32604.tif ---\n",
      "\n",
      "--- dem: checking CRS for 8 rasters ---\n",
      "\n",
      "---   usgs_dem_10m_bigisland.tif: EPSG:4326 ---\n",
      "\n",
      "---   usgs_dem_10m_kahoolawe.tif: EPSG:4326 ---\n",
      "\n",
      "---   usgs_dem_10m_kauai.tif: EPSG:4326 ---\n",
      "\n",
      "---   usgs_dem_10m_lanai.tif: EPSG:4326 ---\n",
      "\n",
      "---   usgs_dem_10m_maui.tif: EPSG:4326 ---\n",
      "\n",
      "---   usgs_dem_10m_molokai.tif: EPSG:4326 ---\n",
      "\n",
      "---   usgs_dem_10m_niihau.tif: EPSG:4326 ---\n",
      "\n",
      "---   usgs_dem_10m_oahu.tif: EPSG:4326 ---\n",
      "\n",
      "--- dem: all rasters have valid CRS ---\n",
      "\n",
      "--- dem: mosaic exists; skipping. ---\n",
      "\n",
      "--- dem: reprojecting raster -> EPSG:32604 ---\n",
      "\n",
      "--- Reproject dem completed in 3.95 minutes ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\dem_hi_pacioos_mosaic_32604\\dem_hi_pacioos_mosaic_32604.tif ---\n",
      "\n",
      "--- Step 2a complete: DEM mosaic ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 2a: Mosaic DEM rasters\")\n",
    "\n",
    "in_dir = IN_SOURCE[\"dem_dir\"]\n",
    "out_path = OUT_PREP[\"dem_mosaic\"]\n",
    "\n",
    "log(f\"Input dir: {in_dir}\")\n",
    "log(f\"Output:    {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 2a.\")\n",
    "else:\n",
    "    try:\n",
    "        # Check CRS of source rasters\n",
    "        crs_check = check_raster_crs(in_dir, \"dem\")\n",
    "        \n",
    "        if crs_check[\"missing\"]:\n",
    "            raise ValueError(f\"DEM rasters missing CRS: {crs_check['missing']}. Fix source files first.\")\n",
    "        \n",
    "        # Check if reprojection needed after mosaic\n",
    "        source_crs_values = set(crs_check[\"crs_values\"].values())\n",
    "        needs_reproject = not all(\"32604\" in str(crs) or \"UTM zone 4N\" in str(crs) for crs in source_crs_values)\n",
    "        \n",
    "        if needs_reproject:\n",
    "            # Mosaic to interim, then reproject to prepared\n",
    "            mosaic_path = INTERIM / \"tempspace/dem_hi_pacioos_mosaic.tif\"\n",
    "            mosaic_rasters(in_dir, mosaic_path, \"dem\", force=FORCE, dry_run=DRY_RUN)\n",
    "            ensure_raster_crs(mosaic_path, out_path, 32604, \"dem\", force=FORCE, dry_run=DRY_RUN)\n",
    "        else:\n",
    "            # Mosaic directly to prepared (already in target CRS)\n",
    "            mosaic_rasters(in_dir, out_path, \"dem\", force=FORCE, dry_run=DRY_RUN)\n",
    "        \n",
    "        log(\"Step 2a complete: DEM mosaic ready\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 2a: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b0e00",
   "metadata": {},
   "source": [
    "#### Step 2b: Water table Mosaic\n",
    "- Inputs: `data/01_inputs/source/watertable_hi_hcpt/*.tif`\n",
    "- Output: `data/01_inputs/prepared/watertable_hi_hcpt_mosaic_32604.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb1b5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2b: Mosaic water table rasters ---\n",
      "\n",
      "--- Input dir: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\watertable_hi_hcpt ---\n",
      "\n",
      "--- Output:    f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\watertable_hi_hcpt_mosaic_32604\\watertable_hi_hcpt_mosaic_32604.tif ---\n",
      "\n",
      "--- watertable: checking CRS for 4 rasters ---\n",
      "\n",
      "---   BI_wt_prj.tif: EPSG:4326 ---\n",
      "\n",
      "---   kauai_wt_prj.tif: EPSG:4326 ---\n",
      "\n",
      "---   maui_wt_prj.tif: EPSG:4326 ---\n",
      "\n",
      "---   oahu_wt_prj.tif: EPSG:4326 ---\n",
      "\n",
      "--- watertable: all rasters have valid CRS ---\n",
      "\n",
      "--- watertable: mosaic exists; skipping. ---\n",
      "\n",
      "--- watertable: reprojecting raster -> EPSG:32604 ---\n",
      "\n",
      "--- Reproject watertable completed in 0.13 seconds ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\watertable_hi_hcpt_mosaic_32604\\watertable_hi_hcpt_mosaic_32604.tif ---\n",
      "\n",
      "--- Step 2b complete: water table mosaic ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 2b: Mosaic water table rasters\")\n",
    "\n",
    "in_dir = IN_SOURCE[\"watertable_dir\"]\n",
    "out_path = OUT_PREP[\"watertable_mosaic\"]\n",
    "\n",
    "log(f\"Input dir: {in_dir}\")\n",
    "log(f\"Output:    {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 2b.\")\n",
    "else:\n",
    "    try:\n",
    "        # Check CRS of source rasters\n",
    "        crs_check = check_raster_crs(in_dir, \"watertable\")\n",
    "        \n",
    "        if crs_check[\"missing\"]:\n",
    "            raise ValueError(f\"Water table rasters missing CRS: {crs_check['missing']}. Fix source files first.\")\n",
    "        \n",
    "        # Check if reprojection needed after mosaic\n",
    "        source_crs_values = set(crs_check[\"crs_values\"].values())\n",
    "        needs_reproject = not all(\"32604\" in str(crs) or \"UTM zone 4N\" in str(crs) for crs in source_crs_values)\n",
    "        \n",
    "        if needs_reproject:\n",
    "            # Mosaic to interim, then reproject to prepared\n",
    "            mosaic_path = INTERIM / \"tempspace/watertable_hi_hcpt_mosaic.tif\"\n",
    "            mosaic_rasters(in_dir, mosaic_path, \"watertable\", force=FORCE, dry_run=DRY_RUN)\n",
    "            ensure_raster_crs(mosaic_path, out_path, 32604, \"watertable\", force=FORCE, dry_run=DRY_RUN)\n",
    "        else:\n",
    "            # Mosaic directly to prepared (already in target CRS)\n",
    "            mosaic_rasters(in_dir, out_path, \"watertable\", force=FORCE, dry_run=DRY_RUN)\n",
    "        \n",
    "        log(\"Step 2b complete: water table mosaic ready\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 2b: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206cb65c",
   "metadata": {},
   "source": [
    "#### Step 2c: Slopes\n",
    "\n",
    "- Inputs: `data/01_inputs/source/slope_hi_hcpt/*.tif`\n",
    "- Output: `data/01_inputs/prepared/.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasters pulled from HCPT are incomplete\n",
    "# Prob just need to reproject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bd925",
   "metadata": {},
   "source": [
    "#### Step 2d: Rainfall\n",
    "\n",
    "- Input: `data/01_inputs/source/annual_rainfall_hi_hcpt/Rain_inann.tif`\n",
    "- Output: `data/01_inputs/prepared/annual_rainfall_hi_hcpt_32604.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab9531f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2d: Reproject rainfall raster ---\n",
      "\n",
      "--- Input:  f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\annual_rainfall_hi_hcpt\\Rain_inann.tif ---\n",
      "\n",
      "--- Output: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\annual_rainfall_hi_hcpt_32604\\annual_rainfall_hi_hcpt_32604.tif ---\n",
      "\n",
      "--- rainfall: reprojecting raster -> EPSG:32604 ---\n",
      "\n",
      "--- Reproject rainfall completed in 0.44 seconds ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\annual_rainfall_hi_hcpt_32604\\annual_rainfall_hi_hcpt_32604.tif ---\n",
      "\n",
      "--- Step 2d complete: rainfall raster ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 2d: Reproject rainfall raster\")\n",
    "\n",
    "in_path = IN_SOURCE[\"rainfall\"]\n",
    "out_path = OUT_PREP[\"rainfall\"]\n",
    "\n",
    "log(f\"Input:  {in_path}\")\n",
    "log(f\"Output: {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 2d.\")\n",
    "else:\n",
    "    try:\n",
    "        ensure_raster_crs(\n",
    "            in_path=in_path,\n",
    "            out_path=out_path,\n",
    "            target_epsg=32604,\n",
    "            layer_name=\"rainfall\",\n",
    "            force=FORCE,\n",
    "            dry_run=DRY_RUN,\n",
    "        )\n",
    "        log(\"Step 2d complete: rainfall raster ready\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 2d: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f168e7",
   "metadata": {},
   "source": [
    "### Step 3: Prep vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3b7e5",
   "metadata": {},
   "source": [
    "- Workflow:\n",
    "    - Load > validate geometries > reproject to EPSG:32604 if needed > geometry QA > export GPKG\n",
    "- Sub-steps:\n",
    "    - 3a: Coastline\n",
    "    - 3b: Special Management Area (SMA)\n",
    "    - 3c: Streams\n",
    "    - 3d: Municipal wells\n",
    "    - 3e: Domestic wells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc574957",
   "metadata": {},
   "source": [
    "#### Step 3a: Coastline\n",
    "\n",
    "- Input: `data/01_inputs/source/coastline_hi_op/coastline.shp`\n",
    "- Output: `data/01_inputs/prepared/coastline_hi_op_32604.gpkg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27cce9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3a: Prep coastline vector (source > prepared) ---\n",
      "\n",
      "--- Input:  f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\coastline_hi_op\\coastline.shp ---\n",
      "\n",
      "--- Output: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\coastline_hi_op_32604.gpkg ---\n",
      "\n",
      "--- Reading coastline: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\coastline_hi_op\\coastline.shp ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apc\\miniconda3\\envs\\geo\\Lib\\site-packages\\pyogrio\\core.py:26: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Read coastline completed in 3.48 seconds ---\n",
      "\n",
      "--- coastline: rows=13 | crs=EPSG:3750 ---\n",
      "\n",
      "--- coastline: invalid geometries (before): 0 ---\n",
      "\n",
      "--- coastline: invalid geometries (after):  0 ---\n",
      "\n",
      "--- coastline: reprojecting from EPSG:3750 to EPSG:32604 ---\n",
      "\n",
      "--- Reproject coastline completed in 0.10 seconds ---\n",
      "\n",
      "--- coastline geometry QA: null=0, empty=0, invalid=0 ---\n",
      "\n",
      "--- Write coastline (GPKG) completed in 0.64 seconds ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\coastline_hi_op_32604.gpkg ---\n",
      "\n",
      "--- Step 3a complete: coastline ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 3a: Prep coastline vector (source > prepared)\")\n",
    "\n",
    "in_path = IN_SOURCE[\"coastline\"]\n",
    "out_path = OUT_PREP[\"coastline\"]\n",
    "\n",
    "log(f\"Input:  {in_path}\")\n",
    "log(f\"Output: {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 3a.\")\n",
    "else:\n",
    "    try:\n",
    "        _ = prep_vector(\n",
    "            in_path=in_path,\n",
    "            out_path=out_path,\n",
    "            layer_name=\"coastline\",\n",
    "            target_crs=TARGET_CRS,\n",
    "        )\n",
    "        log(\"Step 3a complete: coastline ready\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 3a: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe3bc2",
   "metadata": {},
   "source": [
    "#### Step 3b: Special Management Area (SMA)\n",
    "\n",
    "- Input: `data/01_inputs/source/sma_hi_op/sma.shp`\n",
    "- Output: `data/01_inputs/prepared/sma_hi_op_32604.gpkg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f119af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3b: Prep SMA vector (source > prepared) ---\n",
      "\n",
      "--- Input:  f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\sma_hi_op\\sma.shp ---\n",
      "\n",
      "--- Output: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\sma_hi_op_32604.gpkg ---\n",
      "\n",
      "--- Reading sma: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\sma_hi_op\\sma.shp ---\n",
      "\n",
      "--- Read sma completed in 0.17 seconds ---\n",
      "\n",
      "--- sma: rows=45 | crs=EPSG:3750 ---\n",
      "\n",
      "--- sma: invalid geometries (before): 2 ---\n",
      "\n",
      "--- sma: make_valid completed in 2.00 seconds ---\n",
      "\n",
      "--- sma: invalid geometries (after):  0 ---\n",
      "\n",
      "--- sma: reprojecting from EPSG:3750 to EPSG:32604 ---\n",
      "\n",
      "--- Reproject sma completed in 0.14 seconds ---\n",
      "\n",
      "--- sma geometry QA: null=0, empty=0, invalid=0 ---\n",
      "\n",
      "--- Write sma (GPKG) completed in 0.61 seconds ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\sma_hi_op_32604.gpkg ---\n",
      "\n",
      "--- Step 3b complete: SMA ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 3b: Prep SMA vector (source > prepared)\")\n",
    "\n",
    "in_path = IN_SOURCE[\"sma\"]\n",
    "out_path = OUT_PREP[\"sma\"]\n",
    "\n",
    "log(f\"Input:  {in_path}\")\n",
    "log(f\"Output: {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 3b.\")\n",
    "else:\n",
    "    try:\n",
    "        _ = prep_vector(\n",
    "            in_path=in_path,\n",
    "            out_path=out_path,\n",
    "            layer_name=\"sma\",\n",
    "            target_crs=TARGET_CRS,\n",
    "        )\n",
    "        log(\"Step 3b complete: SMA ready\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 3b: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872ed31",
   "metadata": {},
   "source": [
    "#### Step 3c: Streams\n",
    "\n",
    "- Inputs: `data/01_inputs/source//*.tif`\n",
    "- Output: `data/01_inputs/prepared/.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c70980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3c: Prep streams (source > prepared) ---\n",
      "\n",
      "--- Input:  f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\streams_hi_hcpt\\Streams_prj.shp ---\n",
      "\n",
      "--- Output: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\streams_hi_hcpt_32604.gpkg ---\n",
      "\n",
      "--- Reading streams: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\streams_hi_hcpt\\Streams_prj.shp ---\n",
      "\n",
      "--- Read streams completed in 0.41 seconds ---\n",
      "\n",
      "--- streams: rows=10,785 | crs=EPSG:4326 ---\n",
      "\n",
      "--- streams: invalid geometries (before): 0 ---\n",
      "\n",
      "--- streams: invalid geometries (after):  0 ---\n",
      "\n",
      "--- streams: reprojecting from EPSG:4326 to EPSG:32604 ---\n",
      "\n",
      "--- Reproject streams completed in 0.19 seconds ---\n",
      "\n",
      "--- streams geometry QA: null=0, empty=0, invalid=0 ---\n",
      "\n",
      "--- Write streams (GPKG) completed in 2.10 seconds ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\streams_hi_hcpt_32604.gpkg ---\n",
      "\n",
      "--- Step 3c complete: streams ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 3c: Prep streams (source > prepared)\")\n",
    "\n",
    "in_path = IN_SOURCE[\"streams\"]\n",
    "out_path = OUT_PREP[\"streams\"]\n",
    "\n",
    "log(f\"Input:  {in_path}\")\n",
    "log(f\"Output: {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 3c.\")\n",
    "else:\n",
    "    try:\n",
    "        _ = prep_vector(in_path, out_path, \"streams\", TARGET_CRS)\n",
    "        log(\"Step 3c complete: streams ready\")\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 3c: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8d78e",
   "metadata": {},
   "source": [
    "#### Step 3d: Municipal Wells\n",
    "\n",
    "- Inputs: `data/01_inputs/source//*.tif`\n",
    "- Output: `data/01_inputs/prepared/.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f817385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3d: Prep municipal wells (source > prepared) ---\n",
      "\n",
      "--- Input:  f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\wells_hi_hcpt\\CWRM_Wells_MUN_prj.shp ---\n",
      "\n",
      "--- Output: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\wells_hi_hcpt_mun_32604.gpkg ---\n",
      "\n",
      "--- Reading mun_wells: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\wells_hi_hcpt\\CWRM_Wells_MUN_prj.shp ---\n",
      "\n",
      "--- Read mun_wells completed in 0.20 seconds ---\n",
      "\n",
      "--- mun_wells: rows=534 | crs=EPSG:4326 ---\n",
      "\n",
      "--- mun_wells: invalid geometries (before): 0 ---\n",
      "\n",
      "--- mun_wells: invalid geometries (after):  0 ---\n",
      "\n",
      "--- mun_wells: reprojecting from EPSG:4326 to EPSG:32604 ---\n",
      "\n",
      "--- Reproject mun_wells completed in 0.00 seconds ---\n",
      "\n",
      "--- mun_wells geometry QA: null=0, empty=0, invalid=0 ---\n",
      "\n",
      "--- Write mun_wells (GPKG) completed in 0.79 seconds ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\wells_hi_hcpt_mun_32604.gpkg ---\n",
      "\n",
      "--- Step 3d complete: mun_wells ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 3d: Prep municipal wells (source > prepared)\")\n",
    "\n",
    "in_path = IN_SOURCE[\"mun_wells\"]\n",
    "out_path = OUT_PREP[\"mun_wells\"]\n",
    "\n",
    "log(f\"Input:  {in_path}\")\n",
    "log(f\"Output: {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 3d.\")\n",
    "else:\n",
    "    try:\n",
    "        _ = prep_vector(in_path, out_path, \"mun_wells\", TARGET_CRS)\n",
    "        log(\"Step 3d complete: mun_wells ready\")\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 3d: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f8816",
   "metadata": {},
   "source": [
    "#### Step 3e: Domestic Wells\n",
    "\n",
    "- Inputs: `data/01_inputs/source//*.tif`\n",
    "- Output: `data/01_inputs/prepared/.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9495a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3e: Prep domestic wells (source > prepared) ---\n",
      "\n",
      "--- Input:  f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\wells_hi_hcpt\\CWRM_Wells_DOM_prj.shp ---\n",
      "\n",
      "--- Output: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\wells_hi_hcpt_dom_32604.gpkg ---\n",
      "\n",
      "--- Reading dom_wells: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\source\\wells_hi_hcpt\\CWRM_Wells_DOM_prj.shp ---\n",
      "\n",
      "--- Read dom_wells completed in 0.19 seconds ---\n",
      "\n",
      "--- dom_wells: rows=910 | crs=EPSG:4326 ---\n",
      "\n",
      "--- dom_wells: invalid geometries (before): 0 ---\n",
      "\n",
      "--- dom_wells: invalid geometries (after):  0 ---\n",
      "\n",
      "--- dom_wells: reprojecting from EPSG:4326 to EPSG:32604 ---\n",
      "\n",
      "--- Reproject dom_wells completed in 0.01 seconds ---\n",
      "\n",
      "--- dom_wells geometry QA: null=0, empty=0, invalid=0 ---\n",
      "\n",
      "--- Write dom_wells (GPKG) completed in 1.15 seconds ---\n",
      "\n",
      "--- Wrote: f:\\projects\\shuler_lab_projects\\HiOSDS-TechSuitabilityAnalysis\\data\\01_inputs\\prepared\\wells_hi_hcpt_dom_32604.gpkg ---\n",
      "\n",
      "--- Step 3e complete: dom_wells ready ---\n"
     ]
    }
   ],
   "source": [
    "log(\"Step 3e: Prep domestic wells (source > prepared)\")\n",
    "\n",
    "in_path = IN_SOURCE[\"dom_wells\"]\n",
    "out_path = OUT_PREP[\"dom_wells\"]\n",
    "\n",
    "log(f\"Input:  {in_path}\")\n",
    "log(f\"Output: {out_path}\")\n",
    "\n",
    "if not should_run(out_path, FORCE):\n",
    "    log(\"Output exists and FORCE=False; skipping Step 3e.\")\n",
    "else:\n",
    "    try:\n",
    "        _ = prep_vector(in_path, out_path, \"dom_wells\", TARGET_CRS)\n",
    "        log(\"Step 3e complete: dom_wells ready\")\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 3e: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3845167",
   "metadata": {},
   "source": [
    "### Step 4: Correct/Align Cesspool TMKs\n",
    "\n",
    "- Workflow:\n",
    "    - 4a: Load and prep cesspools (subset, rename, reproject, filter to class IV)\n",
    "    - 4b: Spatial join cesspools to parcels + deduplication\n",
    "    - 4c: Process hit cases (match_ok, tmk_corrected, hit_but_tmk_exists_elsewhere)\n",
    "    - 4d: Process no-hit cases (geometry_corrected, nearest_corrected, review)\n",
    "    - 4e: Build review table and export\n",
    "    - 4f: QA/QC review map (if needed)\n",
    "\n",
    "See Appendix: TMK Correction Decision Workflow at the end of this notebook for the full decision logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d1d58",
   "metadata": {},
   "source": [
    "#### Step 4a: Load and prep cesspools\n",
    "\n",
    "- Inputs:\n",
    "    - `data/01_inputs/prepared/parcels_hi_higp_validated_32604.gpkg` (from Step 1)\n",
    "    - `data/01_inputs/source/cesspools_inventory_hi_hcpt/2025CP_Expld_V2_clean.shp`\n",
    "- Sub-steps:\n",
    "    - Load raw cesspools layer\n",
    "    - Subset to needed columns, rename\n",
    "    - Reproject to EPSG:32604\n",
    "    - Validate geometries, remove null/empty\n",
    "    - Ensure TMK is string type\n",
    "    - Filter to class IV records only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Step 4a: Load and prep cesspools\")\n",
    "\n",
    "in_path_cp = IN_SOURCE[\"cesspools\"]\n",
    "out_path_corrected = OUT_PREP[\"cesspools_corrected\"]\n",
    "out_path_review = OUT_REVIEW[\"cesspools_review\"]\n",
    "\n",
    "# Config\n",
    "cp_cfg = CONFIG[\"fields\"][\"cesspools\"]\n",
    "cp_tmk_col = cp_cfg[\"tmk_col\"]\n",
    "pr_tmk_col = CONFIG[\"fields\"][\"parcels\"][\"tmk_col\"]\n",
    "thresh_m = CONFIG[\"thresholds\"][\"tmk_correction_distance_m\"]\n",
    "\n",
    "log(f\"Input cesspools: {in_path_cp}\")\n",
    "log(f\"Input parcels:   (from Step 1, {len(parcels):,} rows)\")\n",
    "log(f\"Output corrected: {out_path_corrected}\")\n",
    "log(f\"Output review:    {out_path_review}\")\n",
    "\n",
    "if not should_run(out_path_corrected, FORCE):\n",
    "    log(\"Output exists and FORCE=False; loading from prepared outputs.\")\n",
    "    cesspools_corrected = read_vector(out_path_corrected, \"cesspools_corrected\")\n",
    "    \n",
    "    if out_path_review.exists():\n",
    "        cesspools_review = read_vector(out_path_review, \"cesspools_review\")\n",
    "    else:\n",
    "        cesspools_review = gpd.GeoDataFrame()\n",
    "    \n",
    "    log(f\"Loaded: {len(cesspools_corrected):,} corrected, {len(cesspools_review):,} review\")\n",
    "    SKIP_STEP_4 = True\n",
    "else:\n",
    "    SKIP_STEP_4 = False\n",
    "    \n",
    "    # Load and prep\n",
    "    cesspools = load_and_prep_cesspools(\n",
    "        in_path=in_path_cp,\n",
    "        keep_cols=cp_cfg[\"keep_cols\"],\n",
    "        rename_map=cp_cfg[\"rename_map\"],\n",
    "        tmk_col=cp_tmk_col,\n",
    "        filter_col=cp_cfg[\"filter_col\"],\n",
    "        filter_value=cp_cfg[\"filter_value\"],\n",
    "        target_crs=TARGET_CRS,\n",
    "    )\n",
    "    \n",
    "    # Preconditions check\n",
    "    n_malformed = check_tmk_correction_preconditions(\n",
    "        cesspools, parcels, cp_tmk_col, pr_tmk_col, TARGET_CRS, thresh_m\n",
    "    )\n",
    "    \n",
    "    # Store original count for validation\n",
    "    n_original = len(cesspools)\n",
    "    \n",
    "    log(f\"Step 4a complete: {len(cesspools):,} cesspools ready for TMK correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf859e1",
   "metadata": {},
   "source": [
    "#### Step 4b: Spatial join cesspools to parcels\n",
    "\n",
    "- Sub-steps:\n",
    "    - Join cesspools to parcels using spatial intersection\n",
    "    - Identify hits (landed in parcel) vs no-hits (outside all parcels)\n",
    "    - Deduplicate boundary cases where one cesspool intersects multiple parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c80417",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Step 4b: Spatial join cesspools to parcels\")\n",
    "\n",
    "if SKIP_STEP_4:\n",
    "    log(\"Skipping (outputs exist; loaded in global environment)\")\n",
    "else:\n",
    "    try:\n",
    "        # Spatial join\n",
    "        with timed_step(\"Spatial join\"):\n",
    "            cp_join = gpd.sjoin(\n",
    "                cesspools.copy(),\n",
    "                parcels[[pr_tmk_col, \"geometry\"]].copy(),\n",
    "                how=\"left\",\n",
    "                predicate=\"intersects\",\n",
    "            )\n",
    "        \n",
    "        # Add stable row ID for tracking\n",
    "        cp_join[\"_cp_rowid\"] = range(len(cp_join))\n",
    "        \n",
    "        # Summary\n",
    "        n_joined = len(cp_join)\n",
    "        n_hits = int(cp_join[pr_tmk_col].notna().sum())\n",
    "        n_nohits = int(cp_join[pr_tmk_col].isna().sum())\n",
    "        n_extra = n_joined - n_original\n",
    "        \n",
    "        log(f\"Original: {n_original:,} | Joined: {n_joined:,}\")\n",
    "        log(f\"  Hits: {n_hits:,} | No-hits: {n_nohits:,} | Extra (boundary dupes): {n_extra:,}\")\n",
    "        \n",
    "        # Deduplicate\n",
    "        if n_extra > 0:\n",
    "            log(f\"Deduplicating {n_extra:,} boundary duplicates\")\n",
    "            cp_match_counts = cp_join.groupby(cp_join.index).size()\n",
    "            cp_multi_idx = cp_match_counts[cp_match_counts > 1].index\n",
    "            cp_join[\"_multi_match_flag\"] = cp_join.index.isin(cp_multi_idx)\n",
    "            cp_join = cp_join.loc[~cp_join.index.duplicated(keep=\"first\")].copy()\n",
    "            log(f\"  Cesspools matching multiple parcels: {len(cp_multi_idx):,}\")\n",
    "        else:\n",
    "            cp_join[\"_multi_match_flag\"] = False\n",
    "        \n",
    "        # Validate row count\n",
    "        if len(cp_join) != n_original:\n",
    "            raise ValueError(f\"Row count mismatch after dedup: {len(cp_join):,} vs {n_original:,}\")\n",
    "        \n",
    "        log(f\"Step 4b complete: {len(cp_join):,} rows after dedup\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 4b: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087e833",
   "metadata": {},
   "source": [
    "#### Step 4c: Process hit cases\n",
    "\n",
    "- Sub-steps:\n",
    "    - Compute match flags (is_hit, tmk_match, tmk_cps_in_parcels, tmk_cps_malformed)\n",
    "    - Assign status for hit cases:\n",
    "        - `match_ok`: TMKs match, no action needed\n",
    "        - `tmk_corrected`: TMK mismatch, corrected to parcel TMK (string change only)\n",
    "        - `hit_but_tmk_exists_elsewhere`: Ambiguous case, flagged for review\n",
    "    - Assign `tmk_malformed` status for invalid TMKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a691b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Step 4c: Process hit cases\")\n",
    "\n",
    "if SKIP_STEP_4:\n",
    "    log(\"Skipping (outputs exist; loaded in global environment)\")\n",
    "else:\n",
    "    try:\n",
    "        # Compute flags\n",
    "        log(\"Computing match flags\")\n",
    "        parcels_tmk_set = set(parcels[pr_tmk_col].dropna().unique())\n",
    "        \n",
    "        cp_join[\"is_hit\"] = cp_join[pr_tmk_col].notna()\n",
    "        cp_join[\"tmk_match\"] = cp_join[\"is_hit\"] & (cp_join[cp_tmk_col] == cp_join[pr_tmk_col])\n",
    "        cp_join[\"tmk_cps_in_parcels\"] = cp_join[cp_tmk_col].isin(parcels_tmk_set)\n",
    "        cp_join[\"tmk_cps_malformed\"] = ~cp_join[cp_tmk_col].apply(is_valid_tmk)\n",
    "        \n",
    "        # Initialize output columns\n",
    "        cp_join[\"tmk_validated\"] = cp_join[cp_tmk_col]\n",
    "        cp_join[\"tmk_status\"] = None\n",
    "        \n",
    "        # Malformed TMKs (regardless of hit/no-hit)\n",
    "        cp_join.loc[cp_join[\"tmk_cps_malformed\"], \"tmk_status\"] = \"tmk_malformed\"\n",
    "        \n",
    "        # Hit + TMK match = match_ok\n",
    "        match_ok_mask = cp_join[\"is_hit\"] & cp_join[\"tmk_match\"] & ~cp_join[\"tmk_cps_malformed\"]\n",
    "        cp_join.loc[match_ok_mask, \"tmk_status\"] = \"match_ok\"\n",
    "        \n",
    "        # Hit + mismatch cases\n",
    "        hit_mismatch_mask = cp_join[\"is_hit\"] & ~cp_join[\"tmk_match\"] & ~cp_join[\"tmk_cps_malformed\"]\n",
    "        \n",
    "        # Mismatch where tmk_cps exists elsewhere = ambiguous\n",
    "        hit_mismatch_exists = hit_mismatch_mask & cp_join[\"tmk_cps_in_parcels\"]\n",
    "        cp_join.loc[hit_mismatch_exists, \"tmk_status\"] = \"hit_but_tmk_exists_elsewhere\"\n",
    "        \n",
    "        # Mismatch where tmk_cps doesn't exist = correct to parcel TMK\n",
    "        hit_mismatch_not_exists = hit_mismatch_mask & ~cp_join[\"tmk_cps_in_parcels\"]\n",
    "        cp_join.loc[hit_mismatch_not_exists, \"tmk_status\"] = \"tmk_corrected\"\n",
    "        cp_join.loc[hit_mismatch_not_exists, \"tmk_validated\"] = cp_join.loc[hit_mismatch_not_exists, pr_tmk_col]\n",
    "        \n",
    "        # Summary\n",
    "        n_malformed = int((cp_join[\"tmk_status\"] == \"tmk_malformed\").sum())\n",
    "        n_match_ok = int((cp_join[\"tmk_status\"] == \"match_ok\").sum())\n",
    "        n_tmk_corrected = int((cp_join[\"tmk_status\"] == \"tmk_corrected\").sum())\n",
    "        n_hit_ambiguous = int((cp_join[\"tmk_status\"] == \"hit_but_tmk_exists_elsewhere\").sum())\n",
    "        \n",
    "        log(f\"  tmk_malformed: {n_malformed:,}\")\n",
    "        log(f\"  match_ok: {n_match_ok:,}\")\n",
    "        log(f\"  tmk_corrected: {n_tmk_corrected:,}\")\n",
    "        log(f\"  hit_but_tmk_exists_elsewhere: {n_hit_ambiguous:,}\")\n",
    "        \n",
    "        log(\"Step 4c complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 4c: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db5fa4",
   "metadata": {},
   "source": [
    "#### Step 4d: Process no-hit cases\n",
    "\n",
    "- Sub-steps:\n",
    "    - No-hit where `tmk_cps` exists in parcels > `geometry_corrected` (move point to matching parcel)\n",
    "    - No-hit where `tmk_cps` doesn't exist > find nearest parcel:\n",
    "        - Within threshold > `nearest_corrected` (correct TMK and geometry)\n",
    "        - Beyond threshold > `review_nohit_beyond_threshold` (flagged for review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ce82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Step 4d: Process no-hit cases\")\n",
    "\n",
    "if SKIP_STEP_4:\n",
    "    log(\"Skipping (outputs exist; loaded in global environment)\")\n",
    "else:\n",
    "    try:\n",
    "        nohit_mask = ~cp_join[\"is_hit\"] & ~cp_join[\"tmk_cps_malformed\"]\n",
    "        n_nohits_valid = int(nohit_mask.sum())\n",
    "        log(f\"No-hit cases (excluding malformed): {n_nohits_valid:,}\")\n",
    "        \n",
    "        # Initialize for later use\n",
    "        nohit_review = gpd.GeoDataFrame()\n",
    "        \n",
    "        if n_nohits_valid > 0:\n",
    "            # -----------------------------------------------------------------\n",
    "            # No-hit where tmk_cps exists in parcels = geometry_corrected\n",
    "            # -----------------------------------------------------------------\n",
    "            nohit_tmk_exists = nohit_mask & cp_join[\"tmk_cps_in_parcels\"]\n",
    "            n_geom_correct = int(nohit_tmk_exists.sum())\n",
    "            \n",
    "            if n_geom_correct > 0:\n",
    "                cp_join.loc[nohit_tmk_exists, \"tmk_status\"] = \"geometry_corrected\"\n",
    "                \n",
    "                # Move points to matching parcel\n",
    "                for idx in cp_join.loc[nohit_tmk_exists].index:\n",
    "                    tmk_val = cp_join.loc[idx, cp_tmk_col]\n",
    "                    matching_parcel = parcels.loc[parcels[pr_tmk_col] == tmk_val]\n",
    "                    if len(matching_parcel) > 0:\n",
    "                        new_point = matching_parcel.geometry.iloc[0].representative_point()\n",
    "                        cp_join.loc[idx, \"geometry\"] = new_point\n",
    "                \n",
    "                log(f\"  geometry_corrected: {n_geom_correct:,}\")\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # No-hit where tmk_cps doesn't exist = nearest parcel lookup\n",
    "            # -----------------------------------------------------------------\n",
    "            nohit_tmk_not_exists = nohit_mask & ~cp_join[\"tmk_cps_in_parcels\"]\n",
    "            n_needs_nearest = int(nohit_tmk_not_exists.sum())\n",
    "            \n",
    "            if n_needs_nearest > 0:\n",
    "                log(f\"  Finding nearest parcel for {n_needs_nearest:,} records...\")\n",
    "                \n",
    "                nohit_nearest_input = cp_join.loc[nohit_tmk_not_exists].copy()\n",
    "                drop_cols = [c for c in [\"index_right\", pr_tmk_col] if c in nohit_nearest_input.columns]\n",
    "                if drop_cols:\n",
    "                    nohit_nearest_input = nohit_nearest_input.drop(columns=drop_cols)\n",
    "                \n",
    "                with timed_step(\"Nearest parcel lookup\"):\n",
    "                    nohit_nearest = gpd.sjoin_nearest(\n",
    "                        nohit_nearest_input,\n",
    "                        parcels[[pr_tmk_col, \"geometry\"]].copy(),\n",
    "                        how=\"left\",\n",
    "                        distance_col=\"dist_to_nearest_m\"\n",
    "                    )\n",
    "                \n",
    "                # Deduplicate ties\n",
    "                nohit_nearest = (\n",
    "                    nohit_nearest\n",
    "                    .sort_values([\"dist_to_nearest_m\", \"index_right\"])\n",
    "                    .drop_duplicates(subset=[\"_cp_rowid\"], keep=\"first\")\n",
    "                    .copy()\n",
    "                )\n",
    "                \n",
    "                log(f\"  Distance range: {nohit_nearest['dist_to_nearest_m'].min():.1f}m - {nohit_nearest['dist_to_nearest_m'].max():.1f}m\")\n",
    "                \n",
    "                # Within threshold = nearest_corrected\n",
    "                within_thresh = nohit_nearest[\"dist_to_nearest_m\"] <= thresh_m\n",
    "                n_within = int(within_thresh.sum())\n",
    "                n_beyond = int((~within_thresh).sum())\n",
    "                \n",
    "                if n_within > 0:\n",
    "                    for _, row in nohit_nearest.loc[within_thresh].iterrows():\n",
    "                        cp_idx = cp_join.loc[cp_join[\"_cp_rowid\"] == row[\"_cp_rowid\"]].index[0]\n",
    "                        cp_join.loc[cp_idx, \"tmk_validated\"] = row[pr_tmk_col]\n",
    "                        cp_join.loc[cp_idx, \"tmk_status\"] = \"nearest_corrected\"\n",
    "                        \n",
    "                        matching_parcel = parcels.loc[parcels[pr_tmk_col] == row[pr_tmk_col]]\n",
    "                        if len(matching_parcel) > 0:\n",
    "                            new_point = matching_parcel.geometry.iloc[0].representative_point()\n",
    "                            cp_join.loc[cp_idx, \"geometry\"] = new_point\n",
    "                    \n",
    "                    log(f\"  nearest_corrected: {n_within:,} (within {thresh_m}m)\")\n",
    "                \n",
    "                # Beyond threshold = review\n",
    "                nohit_review = nohit_nearest.loc[~within_thresh].copy()\n",
    "                \n",
    "                if n_beyond > 0:\n",
    "                    for rowid in nohit_review[\"_cp_rowid\"].values:\n",
    "                        cp_idx = cp_join.loc[cp_join[\"_cp_rowid\"] == rowid].index[0]\n",
    "                        cp_join.loc[cp_idx, \"tmk_status\"] = \"review_nohit_beyond_threshold\"\n",
    "                    \n",
    "                    # Add last_digit_off_by_1 flag\n",
    "                    nohit_review[\"last_digit_off_by_1\"] = (\n",
    "                        nohit_review[cp_tmk_col].str[-1].astype(int) -\n",
    "                        nohit_review[pr_tmk_col].str[-1].astype(int)\n",
    "                    ).abs() == 1\n",
    "                    \n",
    "                    n_off_by_1 = int(nohit_review[\"last_digit_off_by_1\"].sum())\n",
    "                    log(f\"  review_nohit_beyond_threshold: {n_beyond:,} (last digit off by 1: {n_off_by_1:,})\")\n",
    "        \n",
    "        log(\"Step 4d complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 4d: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd1f5f",
   "metadata": {},
   "source": [
    "#### Step 4e: Build review table and export\n",
    "\n",
    "- Outputs:\n",
    "    - `data/01_inputs/prepared/cesspools_inventory_hi_hcpt_corrected_32604.gpkg`\n",
    "    - `data/01_inputs/prepared/review_queue/cesspools_inventory_hi_hcpt_review_queue_32604.gpkg`\n",
    "- Sub-steps:\n",
    "    - Combine flagged records into review table\n",
    "    - Add diagnostic columns (distance to claimed parcel, nearest parcel TMK)\n",
    "    - Export corrected cesspools and review table as GPKG\n",
    "    - Export review table as CSV for easier viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41606c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Step 4e: Build review table and export\")\n",
    "\n",
    "if SKIP_STEP_4:\n",
    "    log(\"Skipping (outputs exist; loaded in global environment)\")\n",
    "else:\n",
    "    try:\n",
    "        # Build review table\n",
    "        log(\"Building review table\")\n",
    "        review_statuses = [\"tmk_malformed\", \"hit_but_tmk_exists_elsewhere\", \"review_nohit_beyond_threshold\"]\n",
    "        review_main = cp_join.loc[cp_join[\"tmk_status\"].isin(review_statuses)].copy()\n",
    "        \n",
    "        # Add distance to claimed parcel for ambiguous hit cases\n",
    "        if \"hit_but_tmk_exists_elsewhere\" in review_main[\"tmk_status\"].values:\n",
    "            review_main[\"dist_to_tmk_cps_parcel_m\"] = None\n",
    "            hit_ambig_mask = review_main[\"tmk_status\"] == \"hit_but_tmk_exists_elsewhere\"\n",
    "            for idx in review_main.loc[hit_ambig_mask].index:\n",
    "                tmk_val = review_main.loc[idx, cp_tmk_col]\n",
    "                point_geom = review_main.loc[idx, \"geometry\"]\n",
    "                matching_parcel = parcels.loc[parcels[pr_tmk_col] == tmk_val]\n",
    "                if len(matching_parcel) > 0:\n",
    "                    dist = point_geom.distance(matching_parcel.geometry.iloc[0])\n",
    "                    review_main.loc[idx, \"dist_to_tmk_cps_parcel_m\"] = round(dist, 1)\n",
    "        \n",
    "        # Merge nohit_review columns\n",
    "        if len(nohit_review) > 0:\n",
    "            nohit_review_cols = nohit_review[[\"_cp_rowid\", pr_tmk_col, \"dist_to_nearest_m\", \"last_digit_off_by_1\"]].copy()\n",
    "            nohit_review_cols = nohit_review_cols.rename(columns={pr_tmk_col: \"nearest_tmk_parcels\"})\n",
    "            review_main = review_main.merge(nohit_review_cols, on=\"_cp_rowid\", how=\"left\")\n",
    "        \n",
    "        log(f\"Review table: {len(review_main):,} records\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # PREPARE FINAL OUTPUTS\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        # Corrected layer: keep original columns + tmk_validated + tmk_status\n",
    "        # Apply rename_map to get post-rename column names\n",
    "        export_cols = [cp_cfg[\"rename_map\"].get(c, c) for c in cp_cfg[\"keep_cols\"]]\n",
    "        export_cols += [\"tmk_validated\", \"tmk_status\", \"geometry\"]\n",
    "        \n",
    "        cesspools_corrected = cp_join[export_cols].copy()\n",
    "        cesspools_review = review_main.copy()\n",
    "        \n",
    "        log(f\"Export columns: {[c for c in export_cols if c != 'geometry']}\")\n",
    "        \n",
    "        # Validate row count\n",
    "        if len(cesspools_corrected) != n_original:\n",
    "            raise ValueError(f\"Row count mismatch: {len(cesspools_corrected):,} vs {n_original:,}\")\n",
    "        \n",
    "        # Status summary\n",
    "        log(\"=\" * 60)\n",
    "        log(\"TMK CORRECTION SUMMARY\")\n",
    "        status_counts = cesspools_corrected[\"tmk_status\"].value_counts()\n",
    "        for status, count in status_counts.items():\n",
    "            pct = count / len(cesspools_corrected) * 100\n",
    "            log(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
    "        log(\"=\" * 60)\n",
    "        \n",
    "        # Export\n",
    "        if DRY_RUN:\n",
    "            log(f\"[DRY RUN] Would write: {out_path_corrected}\")\n",
    "            log(f\"[DRY RUN] Would write: {out_path_review}\")\n",
    "        else:\n",
    "            write_vector_gpkg(cesspools_corrected, out_path_corrected, \"cesspools_corrected\")\n",
    "            write_vector_gpkg(cesspools_review, out_path_review, \"cesspools_review\")\n",
    "            \n",
    "            # CSV for easier viewing\n",
    "            review_csv_path = REVIEW / \"cesspools_review.csv\"\n",
    "            cesspools_review.drop(columns=[\"geometry\"]).to_csv(review_csv_path, index=False)\n",
    "            log(f\"Wrote review CSV: {review_csv_path}\")\n",
    "        \n",
    "        log(\"Step 4e complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in Step 4e: {e}\")\n",
    "        if STOP_ON_ERROR:\n",
    "            raise\n",
    "\n",
    "# Final summary (runs for both branches)\n",
    "log(f\"Cesspools corrected: {len(cesspools_corrected):,} rows\")\n",
    "log(f\"Cesspools for review: {len(cesspools_review):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd57613",
   "metadata": {},
   "source": [
    "#### Step 4f: QA/QC review map (if needed)\n",
    "\n",
    "Interactive map for reviewing flagged cases:\n",
    "- Purple/Blue: Hit ambiguous cases (landed in parcel A, TMK claims parcel B)\n",
    "- Orange: No-hit cases where last digit is off by 1 (likely typo)\n",
    "- Red: No-hit cases needing manual review\n",
    "- Yellow: Malformed TMKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a66a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Step 4f: QA/QC review map (optional)\")\n",
    "\n",
    "if len(cesspools_review) > 0:\n",
    "    try:\n",
    "        review_map = build_review_qaqc_map(\n",
    "            review_main=cesspools_review,\n",
    "            parcels=parcels,\n",
    "            web_crs=WEB_CRS,\n",
    "            cp_tmk_col=cp_tmk_col,\n",
    "            pr_tmk_col=pr_tmk_col,\n",
    "            nearest_pr_tmk_col=\"nearest_tmk_parcels\",\n",
    "        )\n",
    "        if review_map is not None:\n",
    "            display(review_map)\n",
    "    except Exception as e:\n",
    "        log(f\"QA/QC map failed (non-blocking): {e}\")\n",
    "else:\n",
    "    log(\"No review cases; skipping QA/QC map.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"QA/QC map inputs: status counts\")\n",
    "display(cesspools_review[\"tmk_status\"].value_counts(dropna=False))\n",
    "\n",
    "log(\"QA/QC map inputs: last_digit_off_by_1 counts (review_nohit only)\")\n",
    "mask_nohit = cesspools_review[\"tmk_status\"] == \"review_nohit_beyond_threshold\"\n",
    "if mask_nohit.any() and \"last_digit_off_by_1\" in cesspools_review.columns:\n",
    "    display(cesspools_review.loc[mask_nohit, \"last_digit_off_by_1\"].value_counts(dropna=False))\n",
    "else:\n",
    "    log(\"No review_nohit_beyond_threshold rows (or last_digit_off_by_1 missing).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b844752",
   "metadata": {},
   "source": [
    "#### Review questions for PI/team\n",
    "\n",
    "*Data quality:*\n",
    "\n",
    "1. How should we handle malformed TMKs (NULL, wrong length, non-numeric)? Options: exclude, attempt correction, or flag for manual lookup?\n",
    "\n",
    "*Ambiguous hit cases:*\n",
    "\n",
    "2. If a cesspool lands in parcel A but `tmk_cps` matches parcel B elsewhere, which is truth: spatial location or attributed TMK? \n",
    "\n",
    "*No-hit cases beyond threshold:*\n",
    "\n",
    "3. If a cesspool exceeds the distance threshold but the nearest TMK differs by only 1 digit in the last position, should we correct? Sequential TMKs typically indicate adjacent parcels.\n",
    "4. Is there a hard maximum distance beyond which we never correct? Current auto-correction threshold: `thresh_m = 25`.\n",
    "\n",
    "*General:*\n",
    "\n",
    "5. For unresolvable review cases: exclude from MPAT, include with flag, or hold for manual correction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a527073",
   "metadata": {},
   "source": [
    "## Prep Summary\n",
    "Table: dataset name, path, CRS, row count / raster metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Prep Summary\")\n",
    "\n",
    "# Vector summaries\n",
    "vector_summary = []\n",
    "for name, path in {**OUT_PREP, **OUT_REVIEW}.items():\n",
    "    if path.suffix in [\".gpkg\", \".shp\"] and path.exists():\n",
    "        vector_summary.append(summarize_vector(path, name))\n",
    "\n",
    "# Raster summaries\n",
    "raster_summary = []\n",
    "for name, path in OUT_PREP.items():\n",
    "    if path.suffix == \".tif\" and path.exists():\n",
    "        raster_summary.append(summarize_raster(path, name))\n",
    "\n",
    "log(\"Vector layers:\")\n",
    "df_vec = pd.DataFrame(vector_summary)\n",
    "display(df_vec)\n",
    "\n",
    "log(\"Raster layers:\")\n",
    "df_ras = pd.DataFrame(raster_summary)\n",
    "# Format size: show GB if > 1000 MB\n",
    "df_ras[\"size\"] = df_ras[\"size_mb\"].apply(\n",
    "    lambda x: f\"{x/1024:.2f} GB\" if x > 1000 else f\"{x:.1f} MB\"\n",
    ")\n",
    "display(df_ras.drop(columns=[\"size_mb\"]))\n",
    "\n",
    "# Cesspool TMK correction status\n",
    "log(\"Cesspool TMK correction status:\")\n",
    "status_summary = (\n",
    "    cesspools_corrected[\"tmk_status\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"status\", \"tmk_status\": \"count\"})\n",
    ")\n",
    "status_summary.columns = [\"status\", \"count\"]\n",
    "status_summary[\"percent\"] = (status_summary[\"count\"] / len(cesspools_corrected) * 100).round(1)\n",
    "display(status_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786fb9d",
   "metadata": {},
   "source": [
    "#### Cesspool TMK Correction Status Definitions\n",
    "\n",
    "| Status | Step | Correction Applied | Description |\n",
    "|--------|------|-------------------|-------------|\n",
    "| `match_ok` | 4c | None | Cesspool landed in parcel and TMKs match. No action needed. |\n",
    "| `tmk_corrected` | 4c | String only | Cesspool landed in parcel but TMKs didn't match; `tmk_cps` didn't exist elsewhere in parcels. Corrected `tmk_validated` to parcel TMK. |\n",
    "| `geometry_corrected` | 4d | Geometry only | Cesspool landed outside all parcels, but `tmk_cps` exists in parcels. Moved point to matching parcel. |\n",
    "| `nearest_corrected` | 4d | String + geometry | Cesspool landed outside all parcels, `tmk_cps` doesn't exist, but nearest parcel within 25m. Corrected both TMK and geometry to nearest parcel. |\n",
    "| `hit_but_tmk_exists_elsewhere` | 4c | None (review) | Cesspool landed in parcel A, but `tmk_cps` matches parcel B elsewhere. Ambiguous. Flagged for manual review. |\n",
    "| `review_nohit_beyond_threshold` | 4d | None (review) | Cesspool landed outside all parcels, `tmk_cps` doesn't exist, and nearest parcel > 25m away. Flagged for manual review. |\n",
    "| `tmk_malformed` | 4c | None (review) | Invalid TMK format (null, wrong length, or non-numeric). Flagged for manual review. |\n",
    "\n",
    "*Notes*:\n",
    "\n",
    "- Review queue: Statuses ending in \"review\" or flagged are exported to `data/01_inputs/review_queue/` for PI/team manual review. \n",
    "- See step 4f for interactive QA/QC review map\n",
    "- See Appendix: TMK Correction Decision Workflow for full logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2896eec",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3a4a3",
   "metadata": {},
   "source": [
    "### TMK Correction Decision Workflow\n",
    "\n",
    "#### Goal\n",
    "Ensure each cesspool has a validated TMK matching the parcel layer.\n",
    "\n",
    "#### Key assumptions\n",
    "- Parcel TMK values (`tmk_parcels`) are authoritative\n",
    "- Output: `cesspools_validated` GeoDataFrame with:\n",
    "  - `tmk_validated` column\n",
    "  - Corrected geometries where needed\n",
    "\n",
    "#### Definitions\n",
    "- Hit: cesspool point lands within a parcel polygon\n",
    "- No-hit: cesspool point falls outside all parcel polygons\n",
    "- TMK match: `tmk_cps` string equals `tmk_parcels` string\n",
    "\n",
    "#### Decision workflow\n",
    "\n",
    "Step 0: TMK data quality check\n",
    "- Parcels (authoritative layer):\n",
    "  - Malformed TMKs filtered upstream in `process_parcels`\n",
    "  - If any remain, raise error\n",
    "- Cesspools (layer to be corrected):\n",
    "  - Is `tmk_cps` valid? (not null, 9 digits, numeric only)\n",
    "    - Valid: continue to Step 1\n",
    "    - Invalid: flag = `tmk_malformed`\n",
    "\n",
    "Step 1: Spatial join\n",
    "- Does cesspool intersect a parcel polygon?\n",
    "  - Hit: go to Step 2a\n",
    "  - No-hit: go to Step 2b\n",
    "\n",
    "Step 2a: Hit cases (geometry is fine)\n",
    "- Does `tmk_cps` == `tmk_parcels`?\n",
    "  - Yes: status = `match_ok`, no action needed\n",
    "  - No: does `tmk_cps` exist elsewhere in parcels layer?\n",
    "    - Yes: flag = `hit_but_tmk_exists_elsewhere`. Ambiguous case where point landed in parcel A, but `tmk_cps` matches parcel B.\n",
    "    - No: status = `tmk_corrected`. Set `tmk_validated` = `tmk_parcels`. String change only.\n",
    "\n",
    "Step 2b: No-hit cases (geometry needs correction)\n",
    "- Does `tmk_cps` exist anywhere in parcels layer?\n",
    "  - Yes: status = `geometry_corrected`. Move point to parcel matching `tmk_cps`. No string change.\n",
    "  - No: find nearest parcel, check distance\n",
    "    - Distance <= threshold: status = `nearest_corrected`. Correct both string and geometry to nearest parcel.\n",
    "    - Distance > threshold: status = `review_nohit_beyond_threshold`. Add `last_digit_off_by_1` flag column. Output to review table.\n",
    "\n",
    "#### Status categories\n",
    "\n",
    "| Status | Correction type | Description |\n",
    "|--------|-----------------|-------------|\n",
    "| `match_ok` | None | TMKs match, no correction needed |\n",
    "| `tmk_corrected` | String only | Hit, corrected `tmk_cps` to `tmk_parcels` |\n",
    "| `geometry_corrected` | Geometry only | No-hit, `tmk_cps` exists in parcels, moved point |\n",
    "| `nearest_corrected` | String + geometry | No-hit, corrected to nearest parcel within threshold |\n",
    "| `tmk_malformed` | None (flagged) | Invalid TMK format, needs manual review |\n",
    "| `hit_but_tmk_exists_elsewhere` | None (flagged) | Hit parcel A, but `tmk_cps` matches parcel B |\n",
    "| `review_nohit_beyond_threshold` | None (flagged) | No-hit, beyond threshold, needs manual review |\n",
    "\n",
    "#### Review subset flags\n",
    "\n",
    "| Flag/Status | Condition |\n",
    "|-------------|-----------|\n",
    "| `tmk_malformed` | NULL, wrong length (not 9), or non-numeric |\n",
    "| `hit_but_tmk_exists_elsewhere` | Hit parcel A, but `tmk_cps` matches parcel B |\n",
    "| `review_nohit_beyond_threshold` | No-hit, beyond threshold |\n",
    "| `last_digit_off_by_1` | Boolean column: nearest TMK off by 1 in last digit |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
